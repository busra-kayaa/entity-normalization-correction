# ğŸ“š TERMINOLOGY GLOSSARY
## Entity Normalization & Correction

| **Son GÃ¼ncelleme** | 19.02.2026 |
|:-------------------|:-----------|
| **Toplam Terim**   | 38         |
| **HazÄ±rlayan**     | BÃ¼ÅŸra Kaya |

---

## ğŸ“‹ Ä°Ã‡Ä°NDEKÄ°LER

| # | BÃ¶lÃ¼m                                               | Durum |
|:--|:----------------------------------------------------|:------|
| 1 | [STEP 1 - PROBLEM TANIMI](#step-1---problem-tanimi) | âœ… |
| 2 | [STEP 2 - VERÄ° TOPLAMA](#step-2---veri-toplama)     | âœ… |
| 3 | [STEP 3 - GÃœRÃœLTÃœ EKLEME](#step-3---gÃ¼rÃ¼ltÃ¼-ekleme) | âœ… |

---

## STEP 1 - PROBLEM TANIMI
*LiteratÃ¼r taramasÄ± ve problem tanÄ±mÄ± sÃ¼recinde Ã¶ÄŸrenilen terimler*

---

### ğŸ“Œ De-asciification

| | |
| :--- |:---|
| **ğŸ—“ï¸ Ne zaman?** | 11.02.2026 |
| **ğŸ“ Nerede?** | Problem tanÄ±mÄ±, YazÄ±m HatalarÄ± SÄ±nÄ±flandÄ±rmasÄ± |
| **â“ Ne iÅŸe yarar?** | ASCII karakterlere dÃ¶nÃ¼ÅŸmÃ¼ÅŸ TÃ¼rkÃ§e harfleri orijinal haline getirir |
| **ğŸ’¡ Basit Ã¶rnek** | `Turkiye` â†’ **`TÃ¼rkiye`**, `Istanbul` â†’ **`Ä°stanbul`**, `Erdogan` â†’ **`ErdoÄŸan`** |
| **ğŸ“š Benzer terimler** | Turkish character normalization, ASCII conversion, Unicode normalization |

---

### ğŸ“Œ OOV
*Out-of-Vocabulary*

| | |
| :--- |:---|
| **ğŸ—“ï¸ Ne zaman?** | 11.02.2026 |
| **ğŸ“ Nerede?** | Problem KÄ±sÄ±tlarÄ± (KÄ±sÄ±t 3) |
| **â“ Ne iÅŸe yarar?** | Modelin eÄŸitim verisinde **olmayan** kelimeleri ifade eder |
| **ğŸ’¡ Basit Ã¶rnek** | Yeni seÃ§ilen bir bakanÄ±n soyadÄ±, yeni kurulan bir ÅŸirket, yapay isimler |
| **ğŸ“š Benzer terimler** | Unknown words, Unseen tokens, Rare words |

---

### ğŸ“Œ Entity Normalization

| | |
| :--- | :--- |
| **ğŸ—“ï¸ Ne zaman?** | 11.02.2026 |
| **ğŸ“ Nerede?** | Problem tanÄ±mÄ±, YazÄ±m HatalarÄ± SÄ±nÄ±flandÄ±rmasÄ± |
| **â“ Ne iÅŸe yarar?** | AynÄ± varlÄ±ÄŸÄ±n farklÄ± yazÄ±mlarÄ±nÄ± standart forma getirir |
| **ğŸ’¡ Basit Ã¶rnek** | `Turkiye`, `Turkey`, `TURKIYE`, `TÃ¼rkiye Cumhuriyeti` â†’ **`TÃ¼rkiye`** |
| **ğŸ§  Neden gerekli?** | Bilgisayar `Apple`, `Apple Inc.`, `apple`'Ä± **4 farklÄ± ÅŸirket** sanar. Normalizasyon hepsini tek ID altÄ±nda birleÅŸtirir. |
| **ğŸ› ï¸ NasÄ±l Ã§alÄ±ÅŸÄ±r?** | **1. YazÄ±msal temizlik:** BÃ¼yÃ¼k/kÃ¼Ã§Ã¼k harf, noktalama, boÅŸluk <br> **2. SÃ¶zlÃ¼k eÅŸleÅŸtirme:** `ABD` = `USA` = `Amerika` <br> **3. Belirsizlik Ã§Ã¶zÃ¼mÃ¼:** "Amazon" â†’ orman mÄ±? ÅŸirket mi? |
| **ğŸ¯ Entity Linking farkÄ±?** | **Normalization:** Standart forma sokar (`RTE` â†’ `Recep Tayyip ErdoÄŸan`) <br> **Linking:** Bilgi bankasÄ±na baÄŸlar (`Recep Tayyip ErdoÄŸan` â†’ `wikidata.org/Q57418`) |
| **ğŸ“š Benzer terimler** | Entity resolution, Entity linking, Entity alignment, Name normalization, Record linkage |


---

### ğŸ“Œ NER
*Named Entity Recognition*

| |                                                                        |
| :--- |:-----------------------------------------------------------------------|
| **ğŸ—“ï¸ Ne zaman?** | 11.02.2026                                                             |
| **ğŸ“ Nerede?** | Problem tanÄ±mÄ±, Hata SÄ±nÄ±flandÄ±rmasÄ±                                   |
| **â“ Ne iÅŸe yarar?** | Metindeki Ã¶zel isimleri (kiÅŸi, yer, kurum) bulur ve kategorize eder    |
| **ğŸ’¡ Basit Ã¶rnek** | `"Joe Biden Washington'da"` â†’ `[Joe Biden: KÄ°ÅÄ°]`, `[Washington: YER]` |
| **ğŸ“š Benzer terimler** | NER tagging, IOB tagging, Span detection, Entity extraction            |

---

### ğŸ“Œ Context-Aware Spelling Correction

| | |
| :--- |:---|
| **ğŸ—“ï¸ Ne zaman?** | 11.02.2026 |
| **ğŸ“ Nerede?** | Problem tanÄ±mÄ±, Genel Ä°ngilizce YazÄ±m HatalarÄ± |
| **â“ Ne iÅŸe yarar?** | CÃ¼mlenin baÄŸlamÄ±na bakarak yazÄ±m hatalarÄ±nÄ± dÃ¼zeltir |
| **ğŸ’¡ Basit Ã¶rnek** | `"The goverment announced"` â†’ **`"The government announced"`** |
| | `"I have a brown cat"` â†’ âŒ **DÃ¼zeltmez** (Ã§Ã¼nkÃ¼ `"brown"` doÄŸru) |
| **ğŸ“š Benzer terimler** | Grammatical Error Correction (GEC), Spell checking, Proofreading |

### ğŸ“Œ Noisy Text Normalization

| | |
| :--- |:---|
| **ğŸ—“ï¸ Ne zaman?** | 11.02.2026 |
| **ğŸ“ Nerede?** | Problem tanÄ±mÄ±, De-asciification |
| **â“ Ne iÅŸe yarar?** | OCR, klavye, sosyal medya gibi kaynaklardan gelen **kirli metni** temizler |
| **ğŸ’¡ Basit Ã¶rnek** | `"Th3 c@p1t@l 0f Turkiye is Ankara."` â†’ **`"The capital of TÃ¼rkiye is Ankara."`** |
| **ğŸ“š Benzer terimler** | Text cleaning, Text preprocessing, De-asciification, Text denoising |

---

### ğŸ“Œ BERT
*Bidirectional Encoder Representations from Transformers*

| |                                                                                      |
| :--- |:-------------------------------------------------------------------------------------|
| **ğŸ—“ï¸ Ne zaman?** | 12.02.2026                                                                           |
| **ğŸ“ Nerede?** | LiteratÃ¼r taramasÄ±                                                                   |
| **â“ Ne iÅŸe yarar?** | Metindeki kelimeleri Ã§ift yÃ¶nlÃ¼ okuyarak baÄŸlamÄ± anlar                               |
| **ğŸ’¡ Basit Ã¶rnek** | `"ErdoÄŸan cumhurbaÅŸkanÄ±dÄ±r"` â†’ `"ErdoÄŸan"` kelimesinin kiÅŸi olduÄŸunu baÄŸlamdan anlar |
| **ğŸ“š Benzer terimler** | RoBERTa, ALBERT, DistilBERT, Transformer                                             |

---

### ğŸ“Œ GECToR
*Grammatical Error Correction: Tag, Not Rewrite*

| | |
| :--- | :--- |
| **ğŸ—“ï¸ Ne zaman?** | 12.02.2026 |
| **ğŸ“ Nerede?** | LiteratÃ¼r taramasÄ± |
| **â“ Ne iÅŸe yarar?** | Metni yeniden yazmak yerine **etiket atayarak** hata dÃ¼zeltir |
| **ğŸ’¡ Basit Ã¶rnek** | **Geleneksel yÃ¶ntem (Seq2Seq):** <br> `"goed"` â†’ `"went"` (tÃ¼m kelimeyi sil, yeni kelime yaz) <br><br> **GECToR yÃ¶ntemi:** <br> `"goed"` â†’ `"$APPEND_went"` (kelimeye etiket ekle, nasÄ±l dÃ¶nÃ¼ÅŸeceÄŸini sÃ¶yle) |
| **ğŸ§  NasÄ±l Ã§alÄ±ÅŸÄ±r?** | 1. BERT cÃ¼mleyi vektÃ¶rlere Ã§evirir <br> 2. Her kelime iÃ§in bir **etiket** tahmin eder <br> 3. Etiketler dÃ¶nÃ¼ÅŸÃ¼m kuralÄ±nÄ± belirtir: <br> &nbsp;&nbsp;&nbsp; â€¢ `$KEEP` â†’ olduÄŸu gibi bÄ±rak <br> &nbsp;&nbsp;&nbsp; â€¢ `$DELETE` â†’ sil <br> &nbsp;&nbsp;&nbsp; â€¢ `$APPEND_went` â†’ sonuna "went" ekle <br> &nbsp;&nbsp;&nbsp; â€¢ `$REPLACE_went` â†’ "went" ile deÄŸiÅŸtir |
| **âš¡ Seq2Seq'ten farkÄ±?** | **Seq2Seq:** TÃ¼m cÃ¼mleyi **sÄ±fÄ±rdan Ã¼retir** â†’ yavaÅŸ, pahalÄ±, Ã§ok parametre <br> **GECToR:** Sadece **hatalÄ± kÄ±sma etiket** atar â†’ hÄ±zlÄ±, hafif, az parametre |
| **ğŸ“Š Performans** | â€¢ Seq2Seq'ten **5-10 kat daha hÄ±zlÄ±** <br> â€¢ Daha az veriyle eÄŸitilir <br> â€¢ Benzer veya daha iyi doÄŸruluk |
| **ğŸ¯ Neden Ã¶nemli?** | GerÃ§ek zamanlÄ± dÃ¼zeltme yapabilir. Haber akÄ±ÅŸÄ±nda anÄ±nda Ã§alÄ±ÅŸÄ±r. |
| **ğŸ“š Benzer terimler** | Seq2seq, Transformer, Sequence tagging, BERT |

---

### ğŸ“Œ Transformer
*"Attention Is All You Need" - Vaswani et al. 2017*

| | |
| :--- | :--- |
| **ğŸ—“ï¸ Ne zaman?** | 12.02.2026 |
| **ğŸ“ Nerede?** | LiteratÃ¼r taramasÄ± |
| **â“ Ne iÅŸe yarar?** | **NLP'de devrim yaratan mimari.** <br> â€¢ 2017 Ã¶ncesi: LSTM/RNN (kelimeleri **tek tek** okur, yavaÅŸ) <br> â€¢ 2017 sonrasÄ±: Transformer (kelimeleri **aynÄ± anda** okur, hÄ±zlÄ±) |
| **ğŸ’¡ Basit Ã¶rnek** | **LSTM/RNN:** <br> `"Ben [bugÃ¼n] [okula] [gittim]"` <br> â†’ 1. "Ben" oku â†’ 2. "bugÃ¼n" oku â†’ 3. "okula" oku â†’ 4. "gittim" oku <br> âŒ Ä°lk kelimeyi okurken son kelimeyi bilmez <br> âŒ Uzun cÃ¼mlelerde baÅŸÄ± unutur <br><br> **Transformer:** <br> `"Ben bugÃ¼n okula gittim"` <br> â†’ **TÃ¼m kelimeleri AYNI ANDA gÃ¶rÃ¼r!** <br> âœ… "gittim" kelimesini okurken "Ben" kelimesini de gÃ¶rÃ¼r <br> âœ… BaÄŸlamÄ± tam anlar, hiÃ§bir ÅŸeyi unutmaz |
| **ğŸ§  Self-Attention nedir?** | Her kelimenin, cÃ¼mledeki **diÄŸer tÃ¼m kelimelerle** iliÅŸkisini hesaplama: <br><br> `"O bankaya para yatÄ±rdÄ±."` <br> â†’ "bankaya" kelimesi **"para"** ve **"yatÄ±rdÄ±"** ile gÃ¼Ã§lÃ¼ iliÅŸkili â†’ ğŸ¦ BANKA <br><br> `"Nehir kenarÄ±nda oturdu."` <br> â†’ "kenarÄ±nda" kelimesi **"nehir"** ile gÃ¼Ã§lÃ¼ iliÅŸkili â†’ ğŸŒŠ NEHÄ°R BANKASI <br><br> âœ… **AynÄ± kelime ("banka"), farklÄ± baÄŸlamlarda FARKLI vektÃ¶r temsili alÄ±r!** |
| **ğŸ—ï¸ Mimari yapÄ±sÄ±** | **Encoder-Decoder:** <br> â€¢ **Encoder:** Metni anlar, vektÃ¶re Ã§evirir (BERT) <br> â€¢ **Decoder:** VektÃ¶rden yeni metin Ã¼retir (GPT) <br><br> **Multi-head Attention (Ã‡ok baÅŸlÄ± dikkat):** <br> â€¢ 1. baÅŸ: Kelimelerin **gramer** iliÅŸkisini Ã¶ÄŸrenir <br> â€¢ 2. baÅŸ: Kelimelerin **anlam** iliÅŸkisini Ã¶ÄŸrenir <br> â€¢ 3. baÅŸ: Kelimelerin **konum** iliÅŸkisini Ã¶ÄŸrenir <br> â€¢ 4. baÅŸ: **Zamir** iliÅŸkisini Ã¶ÄŸrenir (o/onun) <br> â€¢ 5. baÅŸ: **ZÄ±tlÄ±k** iliÅŸkisini Ã¶ÄŸrenir <br> â€¢ ... 12+ baÅŸ farklÄ± Ã¶zellik Ã¶ÄŸrenir |
| **ğŸ“Š LSTM vs Transformer** | **LSTM/RNN:** <br> âŒ SÄ±ralÄ± iÅŸlem â†’ 100 kelime = 100 adÄ±m <br> âŒ Uzun cÃ¼mlelerde baÅŸÄ± unutur <br> âŒ Paralel iÅŸleme yok = GPU'yu tam kullanamaz <br> âŒ 500+ kelimede performans dÃ¼ÅŸer <br><br> **Transformer:** <br> âœ… Paralel iÅŸlem â†’ 100 kelime = 1 adÄ±m <br> âœ… 1000+ kelimeyi de hatÄ±rlar <br> âœ… GPU'yu %100 kullanÄ±r, Ã§ok hÄ±zlÄ± <br> âœ… 10.000 kelimeye kadar Ã§Ä±kabilir |
| **ğŸ”§ Kullanan modeller** | â€¢ **BERT** - Sadece Encoder (metin anlama) <br> â€¢ **GPT** - Sadece Decoder (metin Ã¼retme) <br> â€¢ **T5** - Encoder + Decoder (Ã§eviri, Ã¶zet) <br> â€¢ **BART** - Encoder + Decoder (metin onarma) <br> â€¢ **RoBERTa** - BERT'in geliÅŸmiÅŸi <br> â€¢ **GECToR** - BERT + etiketleme |
| **ğŸ“š Benzer terimler** | Self-attention, Multi-head attention, Encoder-Decoder, Positional encoding, BERT, GPT |


---


### ğŸ“Œ Fine-tuning

| | |
| :--- | :--- |
| **ğŸ—“ï¸ Ne zaman?** | 12.02.2026 |
| **ğŸ“ Nerede?** | LiteratÃ¼r taramasÄ± |
| **â“ Ne iÅŸe yarar?** | Ã–nceden eÄŸitilmiÅŸ modeli yeni bir veri setiyle Ã¶zelleÅŸtirme |
| **ğŸ’¡ Basit Ã¶rnek** | Genel BERT â†’ Haber metinleriyle eÄŸitilmiÅŸ BERT |
| **ğŸ“š Benzer terimler** | Transfer learning, Pre-training, Domain adaptation |

---

### ğŸ“Œ OCR
*Optical Character Recognition*

| | |
| :--- | :--- |
| **ğŸ—“ï¸ Ne zaman?** | 12.02.2026 |
| **ğŸ“ Nerede?** | LiteratÃ¼r taramasÄ±, Noisy Text Normalization |
| **â“ Ne iÅŸe yarar?** | TaranmÄ±ÅŸ belgeler, PDF'ler veya fotoÄŸraflardaki yazÄ±larÄ± dijital metne Ã§evirir |
| **ğŸ’¡ Basit Ã¶rnek** | TaranmÄ±ÅŸ bir gazete kupÃ¼rÃ¼ â†’ `"TÃ¼rkiye"` yazÄ±sÄ± bilgisayarda **dÃ¼zenlenebilir metin** olur |
| **âš ï¸ OCR hatalarÄ±** | **Harf karÄ±ÅŸÄ±klÄ±klarÄ±:** <br> â€¢ `"TÃ¼rkiye"` â†’ `"Turkiye"` (Ã¼â†’u) <br> â€¢ `"Ä°stanbul"` â†’ `"Istanbul"` (Ä°â†’I) <br> â€¢ `"ErdoÄŸan"` â†’ `"Erdogan"` (ÄŸâ†’g) <br><br> **Benzer harfler:** <br> â€¢ `"O"` ve `"0"` karÄ±ÅŸmasÄ± <br> â€¢ `"l"` (kÃ¼Ã§Ã¼k L) ve `"1"` (bir) karÄ±ÅŸmasÄ± <br> â€¢ `"rn"` â†’ `"m"` olarak okunmasÄ± |
| **ğŸ§  Projeyle iliÅŸkisi** | OCR hatalarÄ± = **Noisy Text**'in ana kaynaklarÄ±ndan biri <br> â€¢ Gazete arÅŸivleri (taranmÄ±ÅŸ) <br> â€¢ Eski belgeler, PDF'ler <br> â€¢ DijitalleÅŸtirilmiÅŸ haber metinleri |
| **ğŸ”§ Ã‡Ã¶zÃ¼m** | **De-asciification** + **Context-Aware Spelling Correction** ile OCR hatalarÄ± dÃ¼zeltilir |
| **ğŸ“š Benzer terimler** | Document scanning, Text digitization, Document analysis, OCR post-correction |

---

### ğŸ“Œ Multi-Head Attention
*Ã‡ok BaÅŸlÄ± Dikkat MekanizmasÄ±*

| | |
| :--- | :--- |
| **ğŸ—“ï¸ Ne zaman?** | 17.02.2026 |
| **ğŸ“ Nerede?** | LiteratÃ¼r taramasÄ±, Transformer mimarisi |
| **â“ Ne iÅŸe yarar?** | CÃ¼mledeki kelimeler arasÄ±ndaki **farklÄ± iliÅŸki tÃ¼rlerini** aynÄ± anda Ã¶ÄŸrenir |
| **ğŸ’¡ Basit Ã¶rnek** | Bir cÃ¼mleyi **farklÄ± uzmanlara** aynÄ± anda inceletmek gibi: <br> â€¢ 1. uzman: Kim kimi iÅŸaret ediyor? (zamirler) <br> â€¢ 2. uzman: Hangi kelimeler anlamca yakÄ±n? <br> â€¢ 3. uzman: CÃ¼mledeki zÄ±tlÄ±klar neler? <br> â€¢ 4. uzman: Dil bilgisi yapÄ±sÄ± nasÄ±l? |
| **ğŸ§  Neden gerekli?** | Tek bir dikkat mekanizmasÄ± **her ÅŸeyi aynÄ± anda** Ã¶ÄŸrenmek zorunda kalÄ±r. <br> Ã‡ok baÅŸlÄ± dikkat ile **her baÅŸ farklÄ± bir Ã¶zelliÄŸe odaklanÄ±r**, sonra hepsi birleÅŸtirilir. |
| **ğŸ› ï¸ NasÄ±l Ã§alÄ±ÅŸÄ±r?** | **AdÄ±m 1:** Girdi cÃ¼mlesi 8 farklÄ± kopyaya Ã§oÄŸaltÄ±lÄ±r <br> **AdÄ±m 2:** Her kopya farklÄ± bir "dikkat baÅŸlÄ±ÄŸÄ±" tarafÄ±ndan iÅŸlenir <br> **AdÄ±m 3:** Her baÅŸ farklÄ± iliÅŸkiler Ã¶ÄŸrenir <br> **AdÄ±m 4:** TÃ¼m baÅŸlarÄ±n Ã§Ä±ktÄ±larÄ± birleÅŸtirilir |
| **ğŸ”§ Transformer'daki yeri** | â€¢ **BERT:** 12 baÅŸlÄ± dikkat (base model) <br> â€¢ **BERT-large:** 16 baÅŸlÄ± dikkat <br> â€¢ **GPT-3:** 96 baÅŸlÄ± dikkat |
| **ğŸ“š Benzer terimler** | Self-attention, Transformer, Attention mechanism, Encoder-Decoder attention |

---
### ğŸ“Œ Positional Encoding
*Konumsal Kodlama / Pozisyonel Kodlama*

| | |
| :--- | :--- |
| **ğŸ—“ï¸ Ne zaman?** | 17.02.2026 |
| **ğŸ“ Nerede?** | LiteratÃ¼r taramasÄ±, Transformer mimarisi |
| **â“ Ne iÅŸe yarar?** | Transformer modeline **kelimelerin cÃ¼mle iÃ§indeki sÄ±rasÄ±nÄ±** Ã¶ÄŸretir |
| **ğŸ’¡ Basit Ã¶rnek** | **Transformer olmadan Ã¶nce (LSTM/RNN):** <br> Kelimeler sÄ±rayla okunur â†’ sÄ±ra bilgisi **otomatik** Ã¶ÄŸrenilir <br><br> **Transformer'da:** <br> TÃ¼m kelimeler **aynÄ± anda** okunur â†’ "Hangi kelime Ã¶nce geldi?" bilgisini kaybeder <br> â†’ **Positional Encoding** kelimelere "Ben 1. kelimeyim", "Ben 2. kelimeyim" etiketi yapÄ±ÅŸtÄ±rÄ±r |
| **ğŸ§  Neden gerekli?** | `"KÃ¶pek adamÄ± Ä±sÄ±rdÄ±"` ile `"Adam kÃ¶peÄŸi Ä±sÄ±rdÄ±"` aynÄ± kelimeler ama **tam tersi anlam**! <br> SÄ±ra bilgisi olmadan Transformer bunlarÄ± **ayÄ±rt edemez**. |
| **ğŸ› ï¸ NasÄ±l Ã§alÄ±ÅŸÄ±r?** | **AdÄ±m 1:** Her kelimeye bir vektÃ¶r atanÄ±r (anlamÄ±) <br> **AdÄ±m 2:** Kelimenin cÃ¼mledeki sÄ±rasÄ±na gÃ¶re bir **pozisyon vektÃ¶rÃ¼** hesaplanÄ±r <br> **AdÄ±m 3:** Anlam vektÃ¶rÃ¼ + Pozisyon vektÃ¶rÃ¼ = Transformer'a giren **son vektÃ¶r** <br><br> `["Ben", "okula", "gidiyorum"]` <br> â†’ "Ben" = anlam vektÃ¶rÃ¼ + (1. pozisyon vektÃ¶rÃ¼) <br> â†’ "okula" = anlam vektÃ¶rÃ¼ + (2. pozisyon vektÃ¶rÃ¼) <br> â†’ "gidiyorum" = anlam vektÃ¶rÃ¼ + (3. pozisyon vektÃ¶rÃ¼) |
| **ğŸ“ FormÃ¼l (Teknik)** | **SinÃ¼zoidal fonksiyonlar** kullanÄ±lÄ±r: <br> â€¢ Ã‡ift indeksler iÃ§in: `sin(pos/10000^(2i/d))` <br> â€¢ Tek indeksler iÃ§in: `cos(pos/10000^(2i/d))` <br><br> *Sebep:* SinÃ¼s ve kosinÃ¼s sayesinde model **gÃ¶receli pozisyonlarÄ±** (2. kelime ile 5. kelime arasÄ±) Ã¶ÄŸrenebilir. |
| **ğŸ”§ Alternatifler** | **Ã–ÄŸrenilebilir Positional Encoding:** <br> â€¢ SinÃ¼zoidal formÃ¼l yerine modelin **kendi Ã¶ÄŸrenmesi** saÄŸlanÄ±r <br> â€¢ GPT gibi modeller bunu kullanÄ±r <br><br> **Relative Positional Encoding:** <br> â€¢ Mutlak sÄ±ra yerine kelimelerin **birbirine gÃ¶re mesafesi** Ã¶ÄŸrenilir |
| **ğŸ“š Benzer terimler** | Self-attention, Transformer, Position embedding, Sinusoidal encoding, Relative position bias |

---
### ğŸ“Œ Masked Language Model (MLM)
*Maskeli Dil Modeli*

| | |
| :--- | :--- |
| **ğŸ—“ï¸ Ne zaman?** | 17.02.2026 |
| **ğŸ“ Nerede?** | LiteratÃ¼r taramasÄ±, BERT mimarisi |
| **â“ Ne iÅŸe yarar?** | BERT'in eÄŸitim yÃ¶ntemi. CÃ¼mledeki bazÄ± kelimeleri **maskeler** ve modelden **tahmin etmesini** ister. |
| **ğŸ’¡ Basit Ã¶rnek** | **Orijinal cÃ¼mle:** `"Ä°stanbul TÃ¼rkiye'nin baÅŸkenti deÄŸildir"` <br> **MaskelenmiÅŸ:** `"Ä°stanbul TÃ¼rkiye'nin [MASK] deÄŸildir"` <br> **BERT'in tahmini:** `"baÅŸkenti"` âœ… |
| **ğŸ§  Neden gerekli?** | **Geleneksel modeller (GPT):** Soldan saÄŸa okur, bir sonraki kelimeyi tahmin eder <br> â†’ `"Ben okula ___"` â†’ `"gidiyorum"` <br><br> **BERT'in yaptÄ±ÄŸÄ±:** Ã‡ift yÃ¶nlÃ¼ okur, **ortadaki** kelimeyi tahmin eder <br> â†’ `"Ben ___ gidiyorum"` â†’ `"okula"` |
| **ğŸ› ï¸ NasÄ±l Ã§alÄ±ÅŸÄ±r?** | **AdÄ±m 1:** CÃ¼mledeki kelimelerin **%15'i** rastgele seÃ§ilir <br> **AdÄ±m 2:** SeÃ§ilen kelimelerin: <br> &nbsp;&nbsp;&nbsp; â€¢ %80'i `[MASK]` ile deÄŸiÅŸtirilir <br> &nbsp;&nbsp;&nbsp; â€¢ %10'u rastgele baÅŸka bir kelimeyle deÄŸiÅŸtirilir <br> &nbsp;&nbsp;&nbsp; â€¢ %10'u **aynen korunur** <br> **AdÄ±m 3:** Model maskelenen yerde **hangi kelime olmasÄ± gerektiÄŸini** tahmin eder |
| **ğŸ§ª Neden %100 MASK kullanÄ±lmÄ±yor?** | EÄŸer hep `[MASK]` gÃ¶rÃ¼rse, BERT **gerÃ§ek metinlerde** mask gÃ¶rmeyeceÄŸi iÃ§in ÅŸaÅŸÄ±rÄ±r. <br> â€¢ %80 MASK â†’ Maskeyi tahmin etmeyi Ã¶ÄŸrenir <br> â€¢ %10 rastgele kelime â†’ HatalarÄ± dÃ¼zeltmeyi Ã¶ÄŸrenir <br> â€¢ %10 aynÄ± kelime â†’ Her zaman deÄŸiÅŸmeyeceÄŸini de Ã¶ÄŸrenir |
| **ğŸ”§ BERT'ten sonra gelen modeller** | **RoBERTa:** %15 MASK, ara **hiÃ§ rastgele veya aynÄ± kelime yok**, sadece MASK! <br> **SpanBERT:** Tek kelime yerine **kelime gruplarÄ±nÄ±** maskeler <br> **ELECTRA:** Maskeli kelimeyi tahmin etmek yerine, **hangi kelimenin deÄŸiÅŸtirildiÄŸini** bulur |
| **ğŸ“š Benzer terimler** | Next Sentence Prediction (NSP), Autoregressive model, Denoising autoencoder, SpanBERT, RoBERTa |

---

### ğŸ“Œ Next Sentence Prediction (NSP)
*Sonraki CÃ¼mle Tahmini*

| | |
| :--- | :--- |
| **ğŸ—“ï¸ Ne zaman?** | 17.02.2026 |
| **ğŸ“ Nerede?** | LiteratÃ¼r taramasÄ±, BERT mimarisi |
| **â“ Ne iÅŸe yarar?** | Ä°ki cÃ¼mle arasÄ±ndaki iliÅŸkiyi Ã¶ÄŸrenir. Ä°kinci cÃ¼mlenin birinciyi takip edip etmediÄŸini tahmin eder. |
| **ğŸ’¡ Basit Ã¶rnek** | **CÃ¼mle A:** `"Ä°stanbul Ã§ok kalabalÄ±k bir ÅŸehir."` <br> **CÃ¼mle B:** `"Her gÃ¼n milyonlarca insan trafikte zaman kaybediyor."` <br> **Tahmin:** `IsNext` âœ… (Birbiriyle ilgili) <br><br> **CÃ¼mle A:** `"Ä°stanbul Ã§ok kalabalÄ±k bir ÅŸehir."` <br> **CÃ¼mle B:** `"Kediler Ã§ok sevimli hayvanlardÄ±r."` <br> **Tahmin:** `NotNext` âŒ (AlakasÄ±z) |
| **ğŸ“š Benzer terimler** | Sentence order prediction, Document-level understanding |

---

### ğŸ“Œ Embeddings from Language Models (ELMo)
*Dil Modellerinden Elde Edilen GÃ¶mmeler*

| | |
| :--- | :--- |
| **ğŸ—“ï¸ Ne zaman?** | 17.02.2026 |
| **ğŸ“ Nerede?** | LiteratÃ¼r taramasÄ± |
| **â“ Ne iÅŸe yarar?** | Kelimelere **cÃ¼mle iÃ§indeki baÄŸlama gÃ¶re** vektÃ¶r atar. AynÄ± kelime farklÄ± cÃ¼mlelerde farklÄ± vektÃ¶r temsili alÄ±r. |
| **ğŸ’¡ Basit Ã¶rnek** | **CÃ¼mle 1:** `"I read the book yesterday."` (read = geÃ§miÅŸ zaman) <br> **CÃ¼mle 2:** `"Can you read the letter now?"` (read = ÅŸimdiki zaman) <br><br> **Word2vec/GloVe:** Ä°ki cÃ¼mledeki "read" kelimesine **aynÄ±** vektÃ¶rÃ¼ verir âŒ <br> **ELMo:** Ä°ki cÃ¼mledeki "read" kelimesine **farklÄ±** vektÃ¶rler verir âœ… |
| **ğŸ§  Neden gerekli?** | Word2vec gibi geleneksel yÃ¶ntemler bir kelimenin **tÃ¼m anlamlarÄ±nÄ±** tek bir vektÃ¶re sÄ±kÄ±ÅŸtÄ±rÄ±r. <br> `"banka"` kelimesi: <br> â€¢ "Para Ã§ekmek iÃ§in bankaya gittim" â†’ ğŸ¦ <br> â€¢ "Nehir bankasÄ±nda oturduk" â†’ ğŸŒŠ <br><br> ELMo, cÃ¼mlenin tamamÄ±nÄ± okuyarak kelimenin **o cÃ¼mledeki anlamÄ±nÄ±** yakalar. |
| **ğŸ› ï¸ NasÄ±l Ã§alÄ±ÅŸÄ±r?** | **AdÄ±m 1:** Ã‡ift yÃ¶nlÃ¼ LSTM (biLM) eÄŸitilir: <br> &nbsp;&nbsp;&nbsp; â€¢ Ä°leri LSTM: soldan saÄŸa okuyarak sonraki kelimeyi tahmin eder <br> &nbsp;&nbsp;&nbsp; â€¢ Geri LSTM: saÄŸdan sola okuyarak Ã¶nceki kelimeyi tahmin eder <br> **AdÄ±m 2:** Her kelime iÃ§in 3 katman vektÃ¶r Ã¼retilir: <br> &nbsp;&nbsp;&nbsp; â€¢ Katman 0: Kelimenin kendi embedding'i <br> &nbsp;&nbsp;&nbsp; â€¢ Katman 1: Ä°lk LSTM katmanÄ± Ã§Ä±ktÄ±sÄ± (dil bilgisi, syntax) <br> &nbsp;&nbsp;&nbsp; â€¢ Katman 2: Ä°kinci LSTM katmanÄ± Ã§Ä±ktÄ±sÄ± (anlam, semantics) <br> **AdÄ±m 3:** Bu 3 katmanÄ±n **aÄŸÄ±rlÄ±klÄ± ortalamasÄ±** alÄ±nÄ±r (aÄŸÄ±rlÄ±klar gÃ¶reve gÃ¶re Ã¶ÄŸrenilir) |
| **ğŸ¯ KatmanlarÄ±n gÃ¶revi** | â€¢ **Alt katman (LSTM1):** Dil bilgisi, sÃ¶zdizimi (POS tagging, dependency) <br> â€¢ **Ãœst katman (LSTM2):** Anlam, baÄŸlam (sentiment, QA, NER) <br><br> *Not:* GÃ¶rev neyse, o gÃ¶rev iÃ§in hangi katman daha Ã¶nemliyse aÄŸÄ±rlÄ±ÄŸÄ± artar. |
| **âš™ï¸ Teknik detay** | â€¢ Karakter seviyesinde CNN ile kelime temsili (OOV sorununa Ã§Ã¶zÃ¼m) <br> â€¢ 2 katman biLSTM <br> â€¢ 4096 hidden unit, 512 projection <br> â€¢ 1 milyar kelime benchmark verisiyle eÄŸitilmiÅŸ |
| **ğŸ“š Benzer terimler** | BiLM (Bidirectional Language Model), Contextual embeddings, CoVe, LSTM, Character CNN |

---

### ğŸ“Œ Dynamic Masking
*Dinamik Maskeleme*

| | |
| :--- | :--- |
| **ğŸ—“ï¸ Ne zaman?** | 17.02.2026 |
| **ğŸ“ Nerede?** | LiteratÃ¼r taramasÄ±, BERT eÄŸitim yÃ¶ntemleri |
| **â“ Ne iÅŸe yarar?** | Her eÄŸitim adÄ±mÄ±nda farklÄ± token'larÄ± rastgele maskeleyerek modelin daha Ã§eÅŸitli Ã¶rnekler gÃ¶rmesini saÄŸlar |
| **ğŸ’¡ Basit Ã¶rnek** | **Statik Maskeleme (Eski yÃ¶ntem):** <br> AynÄ± cÃ¼mle her epoch'ta **aynÄ±** maskelenmiÅŸ halde gÃ¶rÃ¼lÃ¼r <br> `"Ä°stanbul [MASK] en kalabalÄ±k ÅŸehridir"` (her seferinde aynÄ±) <br><br> **Dinamik Maskeleme (Yeni yÃ¶ntem):** <br> Her epoch'ta **farklÄ±** maskelenmiÅŸ haller gÃ¶rÃ¼lÃ¼r <br> â€¢ Epoch 1: `"Ä°stanbul [MASK] en kalabalÄ±k ÅŸehridir"` <br> â€¢ Epoch 2: `"[MASK] TÃ¼rkiye'nin en kalabalÄ±k ÅŸehridir"` <br> â€¢ Epoch 3: `"Ä°stanbul TÃ¼rkiye'nin [MASK] kalabalÄ±k ÅŸehridir"` |
| **ğŸ§  Neden gerekli?** | **Statik maskeleme sorunlarÄ±:** <br> â€¢ Token'larÄ±n %40'Ä± **nadiren** maskelenir <br> â€¢ %15'i **hiÃ§** maskelenmez <br> â€¢ Model aynÄ± maskeye alÄ±ÅŸÄ±r, ezberler <br><br> **Dinamik maskeleme avantajlarÄ±:** <br> â€¢ Her token'Ä±n maskelenme olasÄ±lÄ±ÄŸÄ± artar <br> â€¢ 40 epoch sonunda **%99.9** token maskelenir <br> â€¢ Model gerÃ§ek dil kalÄ±plarÄ±nÄ± Ã¶ÄŸrenir, ezberlemez |
| **ğŸ“Š Ä°statistiksel fark** | **Statik:** <br> Bir token'Ä±n maskelenme olasÄ±lÄ±ÄŸÄ± = **%15** (sabit) <br><br> **Dinamik (40 epoch sonra):** <br> Bir token'Ä±n maskelenme olasÄ±lÄ±ÄŸÄ± = **1 - (1-0.15)â´â° â‰ˆ %99.9** |
| **ğŸ“š Benzer terimler** | Masked LM, Static Masking, BERT pre-training, Data augmentation, Token masking |

---

### ğŸ“Œ Multi-Task Fine-Tuning
*Ã‡ok GÃ¶revli Ä°nce Ayar*

| | |
| :--- | :--- |
| **ğŸ—“ï¸ Ne zaman?** | 17.02.2026 |
| **ğŸ“ Nerede?** | LiteratÃ¼r taramasÄ±, model eÄŸitim yÃ¶ntemleri |
| **â“ Ne iÅŸe yarar?** | Tek bir modeli aynÄ± anda birden fazla gÃ¶revde (Ã¶rneÄŸin NER + Spelling Correction + Entity Normalization) eÄŸiterek hem gÃ¶revler arasÄ± bilgi paylaÅŸÄ±mÄ±nÄ± saÄŸlar hem de modelin genel baÅŸarÄ±mÄ±nÄ± artÄ±rÄ±r. |
| **ğŸ’¡ Basit Ã¶rnek** | **Single-Task Fine-Tuning (Tek GÃ¶rev):** <br> â€¢ Model 1: Sadece NER Ã¶ÄŸrenir `(ErdoÄŸan â†’ KÄ°ÅÄ°)` <br> â€¢ Model 2: Sadece Spelling Correction Ã¶ÄŸrenir `(goverment â†’ government)` <br> â€¢ Model 3: Sadece Entity Normalization Ã¶ÄŸrenir `(Turkiye â†’ TÃ¼rkiye)` <br><br> **Multi-Task Fine-Tuning (Ã‡ok GÃ¶rev):** <br> Tek bir model **hepsini aynÄ± anda** Ã¶ÄŸrenir: <br> â€¢ `"Erdogan goverment"` â†’ `"ErdoÄŸan government"` <br> â€¢ AynÄ± model, kelimeyi hem dÃ¼zeltir (`goverment`) hem normalize eder (`Erdogan`) hem de Ã¶zel isim olduÄŸunu bilir (`ErdoÄŸan` = KÄ°ÅÄ°) |
| **ğŸ§  NasÄ±l Ã§alÄ±ÅŸÄ±r?** | **AdÄ±m 1:** Modele aynÄ± anda farklÄ± gÃ¶revler iÃ§in hazÄ±rlanmÄ±ÅŸ veri setleri gÃ¶sterilir <br> **AdÄ±m 2:** Her gÃ¶rev iÃ§in ayrÄ± bir Ã§Ä±kÄ±ÅŸ katmanÄ± (head) eklenir <br> **AdÄ±m 3:** EÄŸitim sÄ±rasÄ±nda gÃ¶revler arasÄ±nda geÃ§iÅŸ yapÄ±lÄ±r veya gÃ¶revler karÄ±ÅŸÄ±k olarak verilir <br> **AdÄ±m 4:** Modelin alt katmanlarÄ± (gÃ¶vde/body) tÃ¼m gÃ¶revler iÃ§in **ortak** Ã¶zellikleri Ã¶ÄŸrenirken, Ã¼st katmanlar (head) gÃ¶revlere Ã¶zgÃ¼ Ã§Ä±ktÄ±lar Ã¼retir |
| **ğŸ¯ AvantajlarÄ±** | â€¢ **Bilgi paylaÅŸÄ±mÄ±:** Bir gÃ¶revde Ã¶ÄŸrenilen Ã¶zellikler diÄŸer gÃ¶reve de fayda saÄŸlar (transfer learning) <br> â€¢ **Verimlilik:** Tek model = daha az bellek, daha az iÅŸlem gÃ¼cÃ¼ <br> â€¢ **Genelleme:** FarklÄ± gÃ¶revler gÃ¶ren model, her bir gÃ¶revde daha saÄŸlam (robust) hale gelir <br> â€¢ **DÃ¼ÅŸÃ¼k kaynaklÄ± diller/gÃ¶revler:** Az verisi olan gÃ¶revler, Ã§ok verisi olan gÃ¶revlerden Ã¶ÄŸrenir |
| **âš¡ ZorluklarÄ±** | â€¢ **Negatif transfer:** GÃ¶revler birbirine zarar verebilir (Ã§ok farklÄ± gÃ¶revler) <br> â€¢ **GÃ¶rev Ã§akÄ±ÅŸmasÄ±:** FarklÄ± gÃ¶revler aynÄ± girdi iÃ§in farklÄ± Ã§Ä±ktÄ± isteyebilir <br> â€¢ **EÄŸitim zorluÄŸu:** GÃ¶revler arasÄ± dengeyi kurmak (loss weighting) hassas ayar gerektirir |
| **ğŸ“š Benzer terimler** | Multi-task learning (MTL), Transfer learning, Joint training, Multi-head architecture, Negative transfer, Loss weighting, Task balancing |

---

### ğŸ“Œ Damerauâ€“Levenshtein Distance
*Damerauâ€“Levenshtein Mesafesi*

| | |
| :--- | :--- |
| **ğŸ—“ï¸ Ne zaman?** | 17.02.2026 |
| **ğŸ“ Nerede?** | LiteratÃ¼r taramasÄ±, yazÄ±m dÃ¼zeltme yÃ¶ntemleri |
| **â“ Ne iÅŸe yarar?** | Ä°ki kelime arasÄ±ndaki **benzerliÄŸi** Ã¶lÃ§er. Bir kelimeyi diÄŸerine Ã§evirmek iÃ§in gereken **minimum iÅŸlem sayÄ±sÄ±nÄ±** hesaplar. |
| **ğŸ’¡ Basit Ã¶rnek** | `"Erdogan"` â†’ `"ErdoÄŸan"` dÃ¶nÃ¼ÅŸÃ¼mÃ¼ iÃ§in: <br> â€¢ `g` â†’ `ÄŸ` (deÄŸiÅŸtirme) <br> â€¢ Toplam iÅŸlem = **1** <br><br> `"recieve"` â†’ `"receive"` dÃ¶nÃ¼ÅŸÃ¼mÃ¼ iÃ§in: <br> â€¢ `ie` â†’ `ei` (yer deÄŸiÅŸtirme) <br> â€¢ Toplam iÅŸlem = **1** |
| **ğŸ§  Levenshtein'den farkÄ±?** | **Levenshtein:** 3 iÅŸlem <br> â€¢ Ekleme (Insert) <br> â€¢ Silme (Delete) <br> â€¢ DeÄŸiÅŸtirme (Substitute) <br><br> **Damerauâ€“Levenshtein:** 4 iÅŸlem âœ… <br> â€¢ Ekleme (Insert) <br> â€¢ Silme (Delete) <br> â€¢ DeÄŸiÅŸtirme (Substitute) <br> â€¢ **Yer deÄŸiÅŸtirme (Transposition)** ğŸ‘ˆ YENÄ°! <br><br> **Ã–rnek:** `"ie"` â†’ `"ei"` (iki harfin yer deÄŸiÅŸtirmesi) |
| **ğŸ“Š Ä°ÅŸlem tÃ¼rleri** | **Ekleme:** `"erdgan"` â†’ `"erdogan"` (o harfi eklendi) <br> **Silme:** `"erdoÄŸann"` â†’ `"erdoÄŸan"` (fazla n silindi) <br> **DeÄŸiÅŸtirme:** `"erdogan"` â†’ `"erdoÄŸan"` (g â†’ ÄŸ) <br> **Yer deÄŸiÅŸtirme:** `"recieve"` â†’ `"receive"` (ie â†’ ei) |
| **ğŸ”§ Nerede kullanÄ±lÄ±r?** | â€¢ **YazÄ±m dÃ¼zeltme:** `"turkiye"` ile `"tÃ¼rkiye"` benzer mi? <br> â€¢ **OCR hata dÃ¼zeltme:** `"TÃ¼rkiye"` â†’ `"Turkiye"` (Ã¼â†’u) <br> â€¢ **De-asciification:** ASCII'ye Ã§evrilmiÅŸ kelimeleri geri getirme <br> â€¢ **Fonetik benzerlik:** `"erdoan"` ile `"erdoÄŸan"` arasÄ±ndaki fark |
| **ğŸ“š Benzer terimler** | Levenshtein distance, Edit distance, Hamming distance, Jaro-Winkler distance, String similarity, Fuzzy matching |

---

### ğŸ“Œ String-to-String
*Dizeden Dizeye / Karakter Dizisinden Karakter Dizisine*

| | |
| :--- | :--- |
| **ğŸ—“ï¸ Ne zaman?** | 17.02.2026 |
| **ğŸ“ Nerede?** | LiteratÃ¼r taramasÄ±, metin iÅŸleme yÃ¶ntemleri |
| **â“ Ne iÅŸe yarar?** | Bir metin parÃ§asÄ±nÄ± (string) baÅŸka bir metin parÃ§asÄ±na dÃ¶nÃ¼ÅŸtÃ¼ren iÅŸlemleri veya algoritmalarÄ± ifade eder. |
| **ğŸ’¡ Basit Ã¶rnek** | **String-to-String iÅŸlemleri:** <br> â€¢ `"Turkiye"` â†’ `"TÃ¼rkiye"` (de-asciification) <br> â€¢ `"goverment"` â†’ `"government"` (spelling correction) <br> â€¢ `"Erdogan"` â†’ `"ErdoÄŸan"` (karakter normalizasyonu) <br> â€¢ `"Joe Biden Washington'da"` â†’ `"[KÄ°ÅÄ°] [YER]'da"` (NER etiketleme) |
| **ğŸ§  KullanÄ±m alanlarÄ±** | â€¢ **Metin normalizasyonu:** FarklÄ± yazÄ±mlarÄ± standart forma getirme <br> â€¢ **YazÄ±m dÃ¼zeltme:** HatalÄ± kelimeleri doÄŸru hale getirme <br> â€¢ **Makine Ã§evirisi:** Bir dilden baÅŸka bir dile Ã§eviri <br> â€¢ **Metin sadeleÅŸtirme:** KarmaÅŸÄ±k metni basitleÅŸtirme <br> â€¢ **Paraphrase:** AynÄ± anlamÄ± farklÄ± kelimelerle ifade etme |
| **ğŸ”§ String-to-String modelleri** | â€¢ **Seq2Seq (Sequence-to-Sequence):** Encoder-Decoder mimarisi ile bir diziyi baÅŸka bir diziye Ã§evirir <br> â€¢ **Transformer:** Self-attention ile daha baÅŸarÄ±lÄ± string-to-string dÃ¶nÃ¼ÅŸÃ¼mler <br> â€¢ **T5 (Text-to-Text Transfer Transformer):** TÃ¼m NLP gÃ¶revlerini string-to-string problemi olarak modeller <br> â€¢ **GPT:** Verilen string'e uygun devam string'i Ã¼retir |
| **ğŸ“š Benzer terimler** | Sequence-to-sequence (Seq2Seq), Text-to-text, String transformation, Text normalization, String rewriting |

---

### ğŸ“Œ Soft-Masked BERT
*YumuÅŸak Maskeli BERT*

| |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
| :--- |:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **ğŸ—“ï¸ Ne zaman?** | 17.02.2026                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
| **ğŸ“ Nerede?** | LiteratÃ¼r taramasÄ±, yazÄ±m dÃ¼zeltme modelleri                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
| **â“ Ne iÅŸe yarar?** | Metin hatalarÄ±nÄ± dÃ¼zeltmek iÃ§in tasarlanmÄ±ÅŸ, **algÄ±lama (detection)** ve **dÃ¼zeltme (correction)** aÄŸlarÄ±nÄ± birleÅŸtiren BERT tabanlÄ± bir modeldir.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| **ğŸ’¡ Basit Ã¶rnek** | **Girdi:** `"BugÃ¼n hava Ã§ok sÄ±cak, dÄ±ÅŸarÄ± Ã§Ä±kmak istyorum."` <br> **Soft-Masked BERT:** <br> â€¢ **AlgÄ±lama aÄŸÄ±:** `"istiyorum"` kelimesinde `"i"` harfinin eksik olduÄŸunu tespit eder <br> â€¢ **YumuÅŸak maskeleme:** HatalÄ± bÃ¶lgeye odaklanÄ±r <br> â€¢ **DÃ¼zeltme aÄŸÄ± (BERT):** `"istiyorum"` olarak dÃ¼zeltir <br> **Ã‡Ä±ktÄ±:** `"BugÃ¼n hava Ã§ok sÄ±cak, dÄ±ÅŸarÄ± Ã§Ä±kmak istiyorum."`                                                                                                                                                                                                                                                                                                                                                   |
| **ğŸ§  Neden gerekli?** | **BERT'in tek baÅŸÄ±na sorunu:** <br> â€¢ BERT, Masked LM ile eÄŸitilirken kelimeleri **rastgele maskeler** <br> â€¢ Bu nedenle **bir kelimenin hatalÄ± olup olmadÄ±ÄŸÄ±nÄ± tespit etme** konusunda zayÄ±ftÄ±r <br><br> **Soft-Masked BERT'in Ã§Ã¶zÃ¼mÃ¼:** <br> â€¢ **AlgÄ±lama aÄŸÄ±** (Bi-GRU) hangi kelimelerin hatalÄ± olduÄŸunu bulur <br> â€¢ **YumuÅŸak maskeleme** ile sadece hatalÄ± bÃ¶lgelere odaklanÄ±lÄ±r <br> â€¢ **DÃ¼zeltme aÄŸÄ± (BERT)** bu odaklanmÄ±ÅŸ bÃ¶lgeleri dÃ¼zeltir                                                                                                                                                                                                                                                                        |
| **ğŸ› ï¸ Mimari yapÄ±sÄ±** | **1. AlgÄ±lama AÄŸÄ± (Detection Network):** <br> â€¢ Bi-GRU (Ã‡ift yÃ¶nlÃ¼ GRU) kullanÄ±r <br> â€¢ Her karakter iÃ§in **hata olasÄ±lÄ±ÄŸÄ±** `(p_i)` hesaplar (0-1 arasÄ±) <br> â€¢ `p_i` 1'e yakÄ±nsa hatalÄ±, 0'a yakÄ±nsa doÄŸru <br><br> **2. YumuÅŸak Maskeleme (Soft-Masking):** <br> â€¢ Girdi embedding'i `(e_i)` ile maskeleme embedding'i `(e_mask)` arasÄ±nda geÃ§iÅŸ yapar <br> â€¢ `e'_i = p_i * e_mask + (1 - p_i) * e_i` <br> â€¢ HatalÄ± bÃ¶lgeler `e_mask`'e yaklaÅŸÄ±r, doÄŸru bÃ¶lgeler orijinal halini korur <br><br> **3. DÃ¼zeltme AÄŸÄ± (Correction Network):** <br> â€¢ BERT tabanlÄ±dÄ±r <br> â€¢ YumuÅŸak maskelenmiÅŸ embedding'leri alÄ±r, doÄŸru karakterleri Ã¼retir <br> â€¢ Ã‡Ä±kÄ±ÅŸta **residual connection** ve **softmax** ile karakter tahmini yapar |
| **âš¡ GÃ¼Ã§lÃ¼ yÃ¶nleri** | â€¢ **Hata tespiti:** BERT'in zayÄ±f olduÄŸu hata bulma iÅŸini Ã¶zel bir aÄŸ ile Ã§Ã¶zer <br> â€¢ **YumuÅŸak geÃ§iÅŸ:** Keskin maskeleme yerine kademeli geÃ§iÅŸ ile daha doÄŸal Ã¶ÄŸrenme <br> â€¢ **UÃ§tan uca eÄŸitim:** TÃ¼m aÄŸ birlikte eÄŸitilir                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
| **ğŸ“š Benzer terimler** | BERT, Masked LM, Bi-GRU, Sequence tagging, Chinese spelling correction (CSC), Error detection, Residual connection                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |

---
### ğŸ“Œ C2C
*Character-to-Character / Karakterden Karaktere*

| | |
| :--- | :--- |
| **ğŸ—“ï¸ Ne zaman?** | 17.02.2026 |
| **ğŸ“ Nerede?** | LiteratÃ¼r taramasÄ±, metin dÃ¼zeltme modelleri |
| **â“ Ne iÅŸe yarar?** | Bir metindeki hatalÄ± karakterleri tespit edip dÃ¼zelten, **her bir karaktere odaklanan** yaklaÅŸÄ±mlarÄ± ifade eder. |
| **ğŸ’¡ Basit Ã¶rnek** | **Ã–rnek 1 - De-asciification:** <br> `"Turkiye"` â†’ `"TÃ¼rkiye"` <br> â€¢ `T` â†’ `T` (doÄŸru) <br> â€¢ `u` â†’ `Ã¼` (hatalÄ± â†’ dÃ¼zelt) <br> â€¢ `r` â†’ `r` (doÄŸru) <br> â€¢ `k` â†’ `k` (doÄŸru) <br> â€¢ `i` â†’ `i` (doÄŸru) <br> â€¢ `y` â†’ `y` (doÄŸru) <br> â€¢ `e` â†’ `e` (doÄŸru) <br><br> **Ã–rnek 2 - Klavye hatasÄ±:** <br> `"Ankara'ya gidiyroum"` â†’ `"Ankara'ya gidiyorum"` <br> â€¢ `A n k a r a ' y a   g i d i y r o u m` (her karakter ayrÄ± iÅŸlenir) <br> â€¢ `r` ve `o` harflerinin yeri deÄŸiÅŸmiÅŸ â†’ `r o` â†’ `o r` olarak dÃ¼zeltilir |
| **ğŸ§  Neden gerekli?** | â€¢ **Kelime seviyesi modeller** bilinmeyen kelimelerde (OOV) baÅŸarÄ±sÄ±z olur <br> â€¢ **C2C modeller** her karakteri tek tek iÅŸleyerek OOV sorununu Ã§Ã¶zer <br> â€¢ Ã–zellikle **TÃ¼rkÃ§e karakter dÃ¶nÃ¼ÅŸÃ¼mleri** (Ã¼,ÄŸ,ÅŸ,Ä±,Ã¶,Ã§) iÃ§in idealdir <br> â€¢ **OCR hatalarÄ±** gibi karakter bazlÄ± bozulmalarda etkilidir |
| **ğŸ› ï¸ KullanÄ±m alanlarÄ±** | â€¢ **De-asciification:** `"Turkiye"` â†’ `"TÃ¼rkiye"`, `"Istanbul"` â†’ `"Ä°stanbul"` <br> â€¢ **OCR dÃ¼zeltme:** Karakter tanÄ±ma hatalarÄ±nÄ± dÃ¼zeltme (`"TÃ¼rkiye"` â†’ `"Turkiye"` gibi) <br> â€¢ **YazÄ±m dÃ¼zeltme:** `"istiyorum"` â†’ `"istiyorum"` <br> â€¢ **Metin normalizasyonu:** FarklÄ± yazÄ±m standartlarÄ±nÄ± birleÅŸtirme <br> â€¢ **Sosyal medya metinleri:** `"naber genÃ§ler nasÄ±l gidiyo"` â†’ `"naber genÃ§ler nasÄ±l gidiyor"` |
| **ğŸ“Š KarÅŸÄ±laÅŸtÄ±rma** | **Word-level (Kelime seviyesi):** <br> `"Erdogan"` kelime olarak aranÄ±r, sÃ¶zlÃ¼kte yoksa dÃ¼zeltemez âŒ <br><br> **C2C (Karakter seviyesi):** <br> `E r d o g a n` karakterleri tek tek iÅŸlenir: <br> â€¢ `E` (doÄŸru), `r` (doÄŸru), `d` (doÄŸru) <br> â€¢ `o` â†’ `Ã¶` olmalÄ±, `g` â†’ `ÄŸ` olmalÄ± âœ… |
| **âš¡ AvantajlarÄ±** | â€¢ **OOV sorunu yok:** HiÃ§ gÃ¶rÃ¼lmemiÅŸ kelimeleri bile dÃ¼zeltebilir <br> â€¢ **Dil baÄŸÄ±msÄ±z:** TÃ¼rkÃ§e, Ä°ngilizce, Ã‡ince fark etmez <br> â€¢ **Esnek:** Her tÃ¼rlÃ¼ karakter hatasÄ±nÄ± yakalar |
| **âš ï¸ DezavantajlarÄ±** | â€¢ **YavaÅŸ:** Kelime seviyesi modellere gÃ¶re daha yavaÅŸ <br> â€¢ **BaÄŸlam zayÄ±f:** Kelimenin anlamÄ±nÄ± tam kavrayamayabilir <br> â€¢ **Dil bilgisi:** CÃ¼mle yapÄ±sÄ±nÄ± anlamakta zorlanÄ±r |
| **ğŸ“š Benzer terimler** | Character-level model, Character-based correction, Sequence labeling, Character CNN, Byte-Pair Encoding (BPE), Subword tokenization, Character embedding |

---
### ğŸ“ŒBidirectional and Auto-Regressive Transformer (BART) 
*Ã‡ift YÃ¶nlÃ¼ ve Otoregresif DÃ¶nÃ¼ÅŸtÃ¼rÃ¼cÃ¼* 

| | |
| :--- | :--- |
| **ğŸ—“ï¸ Ne zaman?** | 18.02.2026 |
| **ğŸ“ Nerede?** | LiteratÃ¼r taramasÄ±, metin Ã¼retim modelleri |
| **â“ Ne iÅŸe yarar?** | Metin oluÅŸturma (Ã¶zetleme, Ã§eviri) ve anlama gÃ¶revleri iÃ§in kullanÄ±lan, **BERT ve GPT'yi birleÅŸtiren** bir modeldir. |
| **ğŸ’¡ Basit Ã¶rnek** | `"UN Chief Says There Is No <mask> in Syria"` â†’ `"UN Chief Says There Is No Plan to Stop Chemical Weapons in Syria"` |
| **ğŸ“š Benzer terimler** | BERT, GPT, T5, RoBERTa, Seq2Seq, Encoder-Decoder |

---

### ğŸ“Œ Automatic Speech Recognition (ASR)
*Otomatik KonuÅŸma TanÄ±ma*

| | |
| :--- | :--- |
| **ğŸ—“ï¸ Ne zaman?** | 18.02.2026 |
| **ğŸ“ Nerede?** | LiteratÃ¼r taramasÄ±, konuÅŸma iÅŸleme |
| **â“ Ne iÅŸe yarar?** | Ä°nsan konuÅŸmasÄ±nÄ± yazÄ±lÄ± metne dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r. |
| **ğŸ’¡ Basit Ã¶rnek** | Ses kaydÄ±: "Merhaba" â†’ `"Merhaba"` |
| **âš ï¸ ASR hatalarÄ±** | Homofonlar, telaffuz farklÄ±lÄ±klarÄ±, arka plan gÃ¼rÃ¼ltÃ¼sÃ¼ |
| **ğŸ“š Benzer terimler** | Speech-to-Text, Voice Recognition, STT |

---

### ğŸ“Œ Word Error Rate (WER)
*Kelime Hata OranÄ±*

| | |
| :--- | :--- |
| **ğŸ—“ï¸ Ne zaman?** | 18.02.2026 |
| **ğŸ“ Nerede?** | LiteratÃ¼r taramasÄ±, ASR deÄŸerlendirme metrikleri |
| **â“ Ne iÅŸe yarar?** | ASR sistemlerinin doÄŸruluÄŸunu Ã¶lÃ§en en yaygÄ±n metriktir. |
| **ğŸ’¡ Basit Ã¶rnek** | Referans: `"BugÃ¼n hava Ã§ok gÃ¼zel"` (4 kelime) <br> ASR: `"BugÃ¼n hav Ã§ok gÃ¼zel"` (1 hata) <br> **WER = 1/4 = %25** |
| **ğŸ§  Hesaplama** | WER = (DeÄŸiÅŸtirme + Silme + Ekleme) / Toplam Kelime |
| **ğŸ“š Benzer terimler** | CER, Accuracy, Precision, Recall |

---

### ğŸ“Œ Heavy Encoder
*AÄŸÄ±r KodlayÄ±cÄ± / YoÄŸun KodlayÄ±cÄ±*

| | |
| :--- | :--- |
| **ğŸ—“ï¸ Ne zaman?** | 18.02.2026 |
| **ğŸ“ Nerede?** | LiteratÃ¼r taramasÄ±, derin Ã¶ÄŸrenme mimarileri |
| **â“ Ne iÅŸe yarar?** | Genellikle **Ã§ok bÃ¼yÃ¼k ve karmaÅŸÄ±k** encoder modellerini ifade eder. BÃ¼yÃ¼k parametre sayÄ±sÄ±, Ã§ok sayÄ±da katman ve yÃ¼ksek iÅŸlem gÃ¼cÃ¼ gerektiren modeller iÃ§in kullanÄ±lan gayriresmÃ® bir terimdir. |
| **ğŸ’¡ Basit Ã¶rnek** | ModernBERT-large (28 katman, 395 milyon parametre) gibi bÃ¼yÃ¼k encoder modelleri â€œheavy encoderâ€ sÄ±nÄ±fÄ±na girer. |
| **ğŸ§  Nerede kullanÄ±lÄ±r?** | â€¢ Uzun metinleri anlama (16.000 tokenâ€™a kadar) <br> â€¢ KarmaÅŸÄ±k doÄŸal dil anlama gÃ¶revleri <br> â€¢ BÃ¼yÃ¼k Ã¶lÃ§ekli metin sÄ±nÄ±flandÄ±rma <br> â€¢ Domain-specific modeller (biyomedikal, klinik, kod) |
| **ğŸ“š Benzer terimler** | Large Encoder, Deep Encoder, Transformer Encoder, BERT-large, ModernBERT |

---
### ğŸ“Œ CANINE
*Character Architecture with No tokenization In Neural Encoders*

| | |
| :--- | :--- |
| **ğŸ—“ï¸ Ne zaman?** | 18.02.2026 |
| **ğŸ“ Nerede?** | LiteratÃ¼r taramasÄ±, tokenization-free modeller |
| **â“ Ne iÅŸe yarar?** | **AÃ§Ä±k tokenizasyon adÄ±mÄ± (BPE, WordPiece, SentencePiece) kullanmayan** Transformer tabanlÄ± bir dil modelidir. DoÄŸrudan Unicode karakter seviyesinde Ã§alÄ±ÅŸÄ±r. |
| **ğŸ’¡ Basit Ã¶rnek** | Girdi: `"hello world"` â†’ Her karakter Unicode kod noktasÄ±na Ã§evrilir: `[104, 101, 108, 108, 111, 32, 119, 111, 114, 108, 100]` â†’ Model doÄŸrudan bu karakter ID'leri ile Ã§alÄ±ÅŸÄ±r. |
| **ğŸ§  NasÄ±l Ã§alÄ±ÅŸÄ±r?** | â€¢ **3 Transformer encoder** kullanÄ±r: <br> &nbsp;&nbsp; 1. **Shallow encoder (ilk):** Karakter embedding'lerini yerel dikkat ile baÄŸlamlandÄ±rÄ±r <br> &nbsp;&nbsp; 2. **Deep encoder:** Downsampling sonrasÄ± normal BERT benzeri derin encoder uygulanÄ±r <br> &nbsp;&nbsp; 3. **Shallow encoder (son):** Upsampling sonrasÄ± final karakter embedding'lerini oluÅŸturur <br> â€¢ **Downsampling:** 4 kat Ã¶rnekleme azaltma ile uzun karakter dizilerini yÃ¶netilebilir hale getirir |
| **ğŸ“Š Varyantlar** | â€¢ **google/canine-c:** Otoregresif karakter kaybÄ± ile Ã¶n eÄŸitim almÄ±ÅŸ model <br> â€¢ **google/canine-s:** Subword kaybÄ± ile Ã¶n eÄŸitim almÄ±ÅŸ model <br> â€¢ Her ikisi de: 12 katman, 768 hidden, 12 baÅŸlÄ±k, 121M parametre |
| **âš¡ Ã–ne Ã§Ä±kan Ã¶zellik** | â€¢ Tokenizer **tamamen opsiyonel** - Python `ord()` ile direkt Ã§alÄ±ÅŸÄ±r <br> â€¢ Maksimum dizi uzunluÄŸu: **2048 karakter** <br> â€¢ mBERT'e kÄ±yasla **%28 daha az parametre** ile TyDi QA'da **+2.8 F1** |
| **ğŸ“š Benzer terimler** | Character-level model, Tokenization-free, ByT5, Charformer, Perceiver, mBERT, Unicode |

---

### ğŸ“Œ Entity Linking
*VarlÄ±k BaÄŸlama*

| | |
| :--- | :--- |
| **ğŸ—“ï¸ Ne zaman?** | 18.02.2026 |
| **ğŸ“ Nerede?** | LiteratÃ¼r taramasÄ±, bilgi Ã§Ä±karÄ±mÄ± |
| **â“ Ne iÅŸe yarar?** | Metinde geÃ§en bir varlÄ±k ifadesini (Ã¶rneÄŸin "TÃ¼rkiye") bir bilgi tabanÄ±ndaki (Ã¶rneÄŸin Wikipedia, Wikidata) **benzersiz bir varlÄ±ÄŸa baÄŸlar**. |
| **ğŸ’¡ Basit Ã¶rnek** | Metin: `"Ankara, TÃ¼rkiye'nin baÅŸkentidir."` <br> â€¢ `"TÃ¼rkiye"` â†’ `wikidata.org/Q43` <br> â€¢ `"Ankara"` â†’ `wikidata.org/Q3640` |
| **ğŸ§  NasÄ±l Ã§alÄ±ÅŸÄ±r?** | 1. **VarlÄ±k tespiti (NER):** VarlÄ±k ifadeleri bulunur <br> 2. **Aday oluÅŸturma:** Bilgi tabanÄ±nda aynÄ±/isimli varlÄ±klar listelenir <br> 3. **BaÄŸlam tabanlÄ± sÄ±ralama:** En uygun aday seÃ§ilir |
| **ğŸ¯ Entity Normalization farkÄ±?** | **Normalization:** `Turkiye` â†’ `TÃ¼rkiye` (yazÄ±m dÃ¼zeltme) <br> **Linking:** `TÃ¼rkiye` â†’ `wikidata.org/Q43` (bilgi tabanÄ±na baÄŸlama) |
| **ğŸ“š Benzer terimler** | Entity Resolution, Entity Disambiguation, Record Linkage, Knowledge Base Population, Wikidata |

---

### ğŸ“Œ Mention Detection
*Bahsetme Tespiti / VarlÄ±k Tespiti*

| | |
| :--- | :--- |
| **ğŸ—“ï¸ Ne zaman?** | 18.02.2026 |
| **ğŸ“ Nerede?** | LiteratÃ¼r taramasÄ±, bilgi Ã§Ä±karÄ±mÄ±, NER |
| **â“ Ne iÅŸe yarar?** | Metin iÃ§inde **bir varlÄ±ÄŸa iÅŸaret eden ifadeleri (mention)** bulur. NER'in bir alt aÅŸamasÄ± veya alternatifidir. |
| **ğŸ’¡ Basit Ã¶rnek** | Metin: `"Joe Biden, Kamala Harris ile Washington'da gÃ¶rÃ¼ÅŸtÃ¼."` <br> **Mention Detection Ã§Ä±ktÄ±sÄ±:** <br> â€¢ `"Joe Biden"` <br> â€¢ `"Kamala Harris"` <br> â€¢ `"Washington"` |
| **ğŸ§  NER ile farkÄ±?** | â€¢ **NER:** VarlÄ±k mention'larÄ±nÄ± bulur + tÃ¼rÃ¼nÃ¼ (kiÅŸi, yer, kurum) de etiketler <br> â€¢ **MD:** Sadece varlÄ±k mention'larÄ±nÄ± bulur, tÃ¼r belirtmek zorunda deÄŸildir |
| **ğŸ“š Benzer terimler** | Named Entity Recognition (NER), Entity Detection, Span Detection, Entity Mention |

---

### ğŸ“Œ Entity Disambiguation
*VarlÄ±k Anlam BelirsizliÄŸi Giderme*

| | |
| :--- | :--- |
| **ğŸ—“ï¸ Ne zaman?** | 18.02.2026 |
| **ğŸ“ Nerede?** | LiteratÃ¼r taramasÄ±, bilgi Ã§Ä±karÄ±mÄ±, Entity Linking |
| **â“ Ne iÅŸe yarar?** | AynÄ± ada sahip farklÄ± varlÄ±klar arasÄ±nda, baÄŸlama bakarak **doÄŸru olanÄ± seÃ§me** iÅŸlemidir. |
| **ğŸ’¡ Basit Ã¶rnek** | Metin: `"Paris Hilton'da kaldÄ±m."` <br> â€¢ **Aday 1:** Paris (Fransa'nÄ±n baÅŸkenti) ğŸ›ï¸ <br> â€¢ **Aday 2:** Paris Hilton (Ã¼nlÃ¼ sosyetik) ğŸ‘¤ <br><br> **ED Ã§Ä±ktÄ±sÄ±:** BaÄŸlamdaki `"Hilton"` kelimesi sayesinde doÄŸru anlamÄ±n **Paris Hilton** olduÄŸunu belirler. |
| **ğŸ§  NasÄ±l Ã§alÄ±ÅŸÄ±r?** | 1. Aday varlÄ±klar belirlenir (Ã¶rneÄŸin "Paris" iÃ§in ÅŸehir ve kiÅŸi) <br> 2. VarlÄ±ÄŸÄ±n geÃ§tiÄŸi cÃ¼mle ve Ã§evresi incelenir <br> 3. BaÄŸlama en uygun aday seÃ§ilir |
| **ğŸ¯ Ä°liÅŸkili terimlerle farkÄ±** | â€¢ **Entity Linking:** Metindeki varlÄ±ÄŸÄ± bilgi tabanÄ±na baÄŸlar (ED'yi iÃ§erir) <br> â€¢ **Entity Disambiguation:** Sadece anlam belirsizliÄŸini Ã§Ã¶zer <br> â€¢ **Word Sense Disambiguation:** Kelimelerin anlamlarÄ±nÄ± Ã§Ã¶zer (varlÄ±k deÄŸil) |
| **ğŸ“š Benzer terimler** | Entity Linking, Word Sense Disambiguation (WSD), Entity Resolution, Name Disambiguation |

---

## STEP 2 - VERÄ° TOPLAMA
*Wikipedia'dan veri Ã§ekme, API kullanÄ±mÄ±, metin iÅŸleme ve veri saklama sÃ¼recinde Ã¶ÄŸrenilen terimler*

---

### ğŸ“Œ Natural Language Toolkit (NLTK)
*DoÄŸal Dil AraÃ§ TakÄ±mÄ±*

| | |
| :--- | :--- |
| **ğŸ—“ï¸ Ne zaman?** | 18.02.2026 |
| **ğŸ“ Nerede?** | STEP 2 - Veri Toplama, Metin Ä°ÅŸleme |
| **â“ Ne iÅŸe yarar?** | Python iÃ§in doÄŸal dil iÅŸleme (NLP) kÃ¼tÃ¼phanesi. Tokenization, stemming, tagging, parsing gibi iÅŸlemler iÃ§in araÃ§lar saÄŸlar. |
| **ğŸ’¡ Basit Ã¶rnek** | `sent_tokenize("Merhaba dÃ¼nya. NasÄ±lsÄ±n?")` â†’ `["Merhaba dÃ¼nya.", "NasÄ±lsÄ±n?"]` |
| **ğŸ§  Projede kullanÄ±mÄ±** | Wikipedia'dan Ã§ekilen metinleri cÃ¼mlelere ayÄ±rmak iÃ§in `sent_tokenize` kullanÄ±ldÄ± |
| **ğŸ“š Benzer terimler** | spaCy, Tokenization, sent_tokenize, word_tokenize, Corpus |

---

### ğŸ“Œ Tokenization
*TokenleÅŸtirme*

| | |
| :--- | :--- |
| **ğŸ—“ï¸ Ne zaman?** | 18.02.2026 |
| **ğŸ“ Nerede?** | STEP 2 - Veri Toplama, Metin Ã–niÅŸleme |
| **â“ Ne iÅŸe yarar?** | Metni daha kÃ¼Ã§Ã¼k parÃ§alara (kelime, cÃ¼mle, alt kelime) ayÄ±rma iÅŸlemi. |
| **ğŸ’¡ Basit Ã¶rnek** | **CÃ¼mle tokenization:** `"NLTK harika. Ã‡ok kullanÄ±ÅŸlÄ±."` â†’ `["NLTK harika.", "Ã‡ok kullanÄ±ÅŸlÄ±."]` <br> **Kelime tokenization:** `"NLTK harika"` â†’ `["NLTK", "harika"]` |
| **ğŸ§  Projede kullanÄ±mÄ±** | Wikipedia makalelerini cÃ¼mlelere ayÄ±rmak iÃ§in `sent_tokenize()` kullanÄ±ldÄ± |
| **ğŸ“š Benzer terimler** | NLTK, spaCy, sent_tokenize, word_tokenize, Subword tokenization (BPE) |

---

### ğŸ“Œ BeautifulSoup

| | |
| :--- | :--- |
| **ğŸ—“ï¸ Ne zaman?** | 18.02.2026 |
| **ğŸ“ Nerede?** | STEP 2 - Veri Toplama, Web Scraping |
| **â“ Ne iÅŸe yarar?** | HTML ve XML dosyalarÄ±nÄ± ayrÄ±ÅŸtÄ±rmak (parse) iÃ§in kullanÄ±lan Python kÃ¼tÃ¼phanesi. Web'den Ã§ekilen sayfalardan veri Ã§Ä±karmayÄ± kolaylaÅŸtÄ±rÄ±r. |
| **ğŸ’¡ Basit Ã¶rnek** | `soup.find_all('p')` â†’ HTML sayfasÄ±ndaki tÃ¼m paragraf (`<p>`) etiketlerini bulur |
| **ğŸ§  Projede kullanÄ±mÄ±** | Wikipedia sayfalarÄ±ndan ana metin iÃ§eriÄŸini Ã§Ä±karmak iÃ§in kullanÄ±ldÄ±. UyarÄ±larÄ± engellemek iÃ§in `GuessedAtParserWarning` filtrelendi. |
| **ğŸ“š Benzer terimler** | HTML parsing, Web scraping, lxml, html.parser, requests |

---

### ğŸ“Œ Rate Limiting
*HÄ±z SÄ±nÄ±rlama*

| | |
| :--- | :--- |
| **ğŸ—“ï¸ Ne zaman?** | 18.02.2026 |
| **ğŸ“ Nerede?** | STEP 2 - Veri Toplama, API KullanÄ±mÄ± |
| **â“ Ne iÅŸe yarar?** | Bir API'ye (uygulama programlama arayÃ¼zÃ¼) belirli bir sÃ¼rede yapÄ±lan istek sayÄ±sÄ±nÄ± sÄ±nÄ±rlayarak sunucunun aÅŸÄ±rÄ± yÃ¼klenmesini veya IP ban'lenmesini engeller. |
| **ğŸ’¡ Basit Ã¶rnek** | `time.sleep(0.25)` â†’ Her istekten sonra 0.25 saniye bekle <br> â†’ Saniyede 4 istekten fazlasÄ± engellenir |
| **ğŸ§  Projede kullanÄ±mÄ±** | Wikipedia'ya hÄ±zlÄ± istek atÄ±p IP ban yememek iÃ§in her sayfa Ã§ekiminden sonra `time.sleep(0.25)` eklendi |
| **ğŸ“š Benzer terimler** | API throttling, Request limiting, time.sleep, Cooldown |

---

### ğŸ“Œ Metadata
*Ãœst Veri*

| | |
| :--- | :--- |
| **ğŸ—“ï¸ Ne zaman?** | 18.02.2026 |
| **ğŸ“ Nerede?** | STEP 2 - Veri Toplama, Veri Saklama |
| **â“ Ne iÅŸe yarar?** | Veri hakkÄ±nda veri. Bir veri setinin kaynaÄŸÄ±, toplanma zamanÄ±, boyutu gibi tanÄ±mlayÄ±cÄ± bilgileri ifade eder. |
| **ğŸ’¡ Basit Ã¶rnek** | `{"text": "TÃ¼rkiye", "source_page": "Turkey", "word_count": 1, "collected_at": "2026-02-18T..."}` |
| **ğŸ§  Projede kullanÄ±mÄ±** | Her cÃ¼mle iÃ§in kaynak sayfa, kelime sayÄ±sÄ±, toplanma zamanÄ± gibi bilgiler JSON dosyasÄ±nda metadata olarak saklandÄ±. |
| **ğŸ“š Benzer terimler** | Data dictionary, Schema, JSON, Data provenance |

---

### ğŸ“Œ JSON
*JavaScript Object Notation*

| | |
| :--- | :--- |
| **ğŸ—“ï¸ Ne zaman?** | 18.02.2026 |
| **ğŸ“ Nerede?** | STEP 2 - Veri Toplama, Veri Saklama |
| **â“ Ne iÅŸe yarar?** | Verileri metin tabanlÄ±, insan tarafÄ±ndan okunabilir formatta saklamak ve taÅŸÄ±mak iÃ§in kullanÄ±lan hafif bir veri deÄŸiÅŸim formatÄ±. |
| **ğŸ’¡ Basit Ã¶rnek** | `{"isim": "TÃ¼rkiye", "nufus": 85000000}` |
| **ğŸ§  Projede kullanÄ±mÄ±** | Toplanan cÃ¼mleler ve metadata JSON formatÄ±nda `dataset_3000_target.json` ve `dataset_latest.json` dosyalarÄ±na kaydedildi. |
| **ğŸ“š Benzer terimler** | XML, YAML, CSV, Data serialization |

---

## STEP 3 - GÃœRÃœLTÃœ EKLEME
*Sentetik hata Ã¼retimi, veri bozma ve hata tespiti sÃ¼recinde Ã¶ÄŸrenilen terimler*

---

### ğŸ“Œ Regular Expressions (Regex)
*DÃ¼zenli Ä°fadeler*

| | |
| :--- | :--- |
| **ğŸ—“ï¸ Ne zaman?** | 19.02.2026 |
| **ğŸ“ Nerede?** | STEP 2 - Veri Toplama, STEP 3 - GÃ¼rÃ¼ltÃ¼ Ekleme |
| **â“ Ne iÅŸe yarar?** | Metin iÃ§inde desen eÅŸleÅŸtirme, arama, deÄŸiÅŸtirme ve ayÄ±klama iÅŸlemleri iÃ§in kullanÄ±lan gÃ¼Ã§lÃ¼ bir dil. |
| **ğŸ’¡ Basit Ã¶rnekler** | **1. Referans temizleme:** <br> `re.sub(r'\[\d+\]', '', text)` <br> `"TÃ¼rkiye[1]"` â†’ `"TÃ¼rkiye"` <br><br> **2. Fazla boÅŸluk temizleme:** <br> `re.sub(r'\s+', ' ', text)` <br> `"Ã‡ok    boÅŸluk   var"` â†’ `"Ã‡ok boÅŸluk var"` <br><br> **3. Kelime sÄ±nÄ±rÄ± ile arama:** <br> `re.search(r'\bgovernment\b', text)` <br> Sadece tam kelime olarak "government" arar. <br><br> **4. E-posta doÄŸrulama:** <br> `r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'` |
| **ğŸ§  Projede kullanÄ±mÄ±** | â€¢ Wikipedia'dan Ã§ekilen metinlerde **referanslarÄ± temizleme** (`\[\d+\]`) <br> â€¢ **Fazla boÅŸluklarÄ±** ve **satÄ±r sonlarÄ±nÄ±** temizleme (`\s+`, `\n+`) <br> â€¢ **Noktalama dÃ¼zeltmeleri** (`\s+\.`, `\s+,`) <br> â€¢ YaygÄ±n hata kalÄ±plarÄ±nÄ± yakalama (Ã¶rneÄŸin `\bgovernment\b`) <br> â€¢ GÃ¼rÃ¼ltÃ¼ ekleme sÄ±rasÄ±nda kelime sÄ±nÄ±rlarÄ±nÄ± koruma |
| **ğŸ”§ Ã–zel karakterler** | **`.`** â†’ Herhangi bir karakter (newline hariÃ§) <br> **`\d`** â†’ Rakam (`[0-9]`) <br> **`\w`** â†’ Harf, rakam, alt Ã§izgi (`[a-zA-Z0-9_]`) <br> **`\s`** â†’ BoÅŸluk karakteri (space, tab, newline) <br> **`^`** â†’ SatÄ±r baÅŸÄ± <br> **`$`** â†’ SatÄ±r sonu <br> **`*`** â†’ 0 veya daha fazla tekrar <br> **`+`** â†’ 1 veya daha fazla tekrar <br> **`?`** â†’ 0 veya 1 tekrar <br> **`{n}`** â†’ n kadar tekrar <br> **`[abc]`** â†’ a, b veya c karakterlerinden biri <br> **`(abc)`** â†’ Gruplama |
| **ğŸ“š Benzer terimler** | Pattern matching, String searching, Text processing, re module (Python), sed, grep, awk |

---

### ğŸ“Œ Noise Quota
*GÃ¼rÃ¼ltÃ¼ KotasÄ±*

| | |
| :--- | :--- |
| **ğŸ—“ï¸ Ne zaman?** | 19.02.2026 |
| **ğŸ“ Nerede?** | STEP 3 - GÃ¼rÃ¼ltÃ¼ Ekleme, Veri HazÄ±rlama |
| **â“ Ne iÅŸe yarar?** | GÃ¼rÃ¼ltÃ¼ ekleme sÃ¼recinde her hata tipi iÃ§in hedeflenen cÃ¼mle sayÄ±sÄ±nÄ± ifade eder. Toplam gÃ¼rÃ¼ltÃ¼lÃ¼ cÃ¼mle sayÄ±sÄ±nÄ±n, hata tiplerinin aÄŸÄ±rlÄ±klarÄ±na gÃ¶re daÄŸÄ±tÄ±lmasÄ±nÄ± saÄŸlar. |
| **ğŸ’¡ Basit Ã¶rnek** | Toplam 1000 cÃ¼mleye %35 gÃ¼rÃ¼ltÃ¼ eklenecek (350 cÃ¼mle). <br> Hata tiplerinin aÄŸÄ±rlÄ±klarÄ±: <br> â€¢ deascii: %30 â†’ quota = 105 cÃ¼mle <br> â€¢ omission: %12 â†’ quota = 42 cÃ¼mle <br> â€¢ insertion: %8 â†’ quota = 28 cÃ¼mle <br> â€¢ transposition: %10 â†’ quota = 35 cÃ¼mle <br> â€¢ substitution: %15 â†’ quota = 52 cÃ¼mle <br> â€¢ space: %5 â†’ quota = 17 cÃ¼mle <br> â€¢ terminology: %8 â†’ quota = 28 cÃ¼mle <br> â€¢ common: %12 â†’ quota = 42 cÃ¼mle |
| **ğŸ§  Neden gerekli?** | â€¢ Her hata tipinden yeterli sayÄ±da Ã¶rnek olmasÄ±nÄ± garanti eder. <br> â€¢ Rastgele seÃ§imde bazÄ± hata tipleri hiÃ§ temsil edilmeyebilir. <br> â€¢ Dengeli ve Ã§eÅŸitli bir veri seti oluÅŸturmayÄ± saÄŸlar. |
| **ğŸ“š Benzer terimler** | Noise ratio, Error distribution, Sampling quota, Stratified sampling, Class balance |

---

## ğŸ“Š Ã–ZET TABLOSU

| Terim                                         | KÄ±saltma   | Ã–ÄŸrenme Tarihi |
|:----------------------------------------------|:-----------|:---------------|
| De-asciification                              | -          | 11.02.2026     |
| Out-of-Vocabulary                             | **OOV**    | 11.02.2026     |
| Entity Normalization                          | -          | 11.02.2026     |
| Named Entity Recognition                      | **NER**    | 11.02.2026     |
| Context-Aware Spelling Correction             | -          | 11.02.2026     |
| Noisy Text Normalization                      | -          | 11.02.2026     |
| BERT                                          | -          | 12.02.2026     |
| GECToR                                        | -          | 12.02.2026     |
| Transformer                                   | -          | 12.02.2026     |
| Fine-tuning                                   | -          | 12.02.2026     |
| OCR                                           | -          | 12.02.2026     |
| Multi-Head Attention                          | -          | 17.02.2026     |
| Positional Encoding                           | -          | 17.02.2026     |
| Masked Language Model                         | **MLM**    | 17.02.2026     |
| Next Sentence Prediction                      | **NSP**    | 17.02.2026     |
| Embeddings from Language Models               | **ELMo**   | 17.02.2026     |
| Dynamic Masking                               | -          | 17.02.2026     |
| Multi-Task Fine-Tuning                        | -          | 17.02.2026     |
| Damerauâ€“Levenshtein Distance                  | **DLD**    | 17.02.2026     |
| String-to-String                              | **S2S**    | 17.02.2026     |
| Soft-Masked BERT                              | -          | 17.02.2026     |
| C2C (Character-to-Character)                  | **C2C**    | 17.02.2026     |
| Bidirectional and Auto-Regressive Transformer | **BART**   | 18.02.2026     |
| Automatic Speech Recognition                  | **ASR**    | 18.02.2026     |
| Word Error Rate                               | **WER**    | 18.02.2026     |
| Heavy Encoder                                 | -          | 18.02.2026     |
| CANINE                                        | **CANINE** | 18.02.2026     |
| Entity Linking                                | **EL**     | 18.02.2026     |
| Mention Detection                             | **MD**     | 18.02.2026     |
| Entity Disambiguation                         | **ED**     | 18.02.2026     |
| Natural Language Toolkit                      | **NLTK**   | 18.02.2026     |
| Tokenization                                  | -          | 18.02.2026     |
| BeautifulSoup                                 | **BS4**    | 18.02.2026     |
| Rate Limiting                                 | -          | 18.02.2026     |
| Metadata                                      | -          | 18.02.2026     |
| JSON                                          | **JSON**   | 18.02.2026     |
| Regular Expressions                           | **Regex**  | 19.02.2026     |
| Noise Quota                                   | -          | 19.02.2026     |

---

## ğŸ“Œ DEÄÄ°ÅÄ°KLÄ°K KAYITLARI

| Tarih | Versiyon | Eklenen Terimler                                              | AÃ§Ä±klama                                                                    |
| :--- |:---------|:--------------------------------------------------------------|:----------------------------------------------------------------------------|
| 11.02.2026 | v1.0     | OOV, NER, Entity Norm, Spelling, Noisy Text, De-asciification | Ä°lk oluÅŸturma                                                               |
| 12.02.2026 | v1.1     | BERT, GECToR, Transformer, Fine-tuning, OCR                   | LiteratÃ¼r taramasÄ± eklendi                                                  |
| 17.02.2026 | v1.2     | Multi-Head Attention, Positional Encoding                     | Transformer detaylandÄ±rÄ±ldÄ±                                                 |
| 17.02.2026 | v1.3     | MLM, NSP, Dynamic Masking                                     | BERT eÄŸitim yÃ¶ntemi eklendi                                                 |
| 17.02.2026 | v1.4     | ELMo                                                          | BaÄŸlamsal embedding modeli eklendi                                          |
| 17.02.2026 | v1.5     | Multi-Task Fine-Tuning                                        | Ã‡ok gÃ¶revli eÄŸitim yÃ¶ntemi eklendi                                          |
| 17.02.2026 | v1.6     | Damerauâ€“Levenshtein Distance                                  | Edit distance metriÄŸi eklendi                                               |
| 17.02.2026 | v1.7     | String-to-String                                              | Metin dÃ¶nÃ¼ÅŸÃ¼m terimi eklendi                                                |
| 17.02.2026 | v1.8     | Soft-Masked BERT                                              | YazÄ±m dÃ¼zeltme modeli eklendi                                               |
| 17.02.2026 | v1.9     | C2C (Character-to-Character)                                  | Karakter seviyesi iÅŸleme terimi eklendi                                     |
| 18.02.2026 | v2.0     | BART, ASR, WER                                                | Metin Ã¼retim modeli, konuÅŸma tanÄ±ma ve hata metriÄŸi eklendi                 |
| 18.02.2026 | v2.1     | Heavy Encoder                                                 | BÃ¼yÃ¼k ve karmaÅŸÄ±k encoder modelleri iÃ§in kullanÄ±lan terim eklendi           |
| 18.02.2026 | v2.2     | CANINE                                                        | Tokenization-free karakter seviyesi model eklendi                           |
| 18.02.2026 | v2.3     | Entity Linking                                                | VarlÄ±k baÄŸlama terimi eklendi                                               |
| 18.02.2026 | v2.4     | Mention Detection (MD)                                        | VarlÄ±k tespiti terimi eklendi                                               |
| 18.02.2026 | v2.5     | Entity Disambiguation (ED)                                    | VarlÄ±k anlam belirsizliÄŸi giderme terimi eklendi                            |
| 18.02.2026 | v2.6     | NLTK, Tokenization, BS4, Rate Limiting, Metadata, JSON        | STEP 2 - Data Collection kapsamÄ±nda kullanÄ±lan araÃ§lar ve kavramlar eklendi |
| 19.02.2026 | v2.7     | Regular Expressions (Regex)                                   | STEP 2 ve STEP 3'te kullanÄ±lan dÃ¼zenli ifadeler eklendi                     |
| 19.02.2026 | v2.8     | Noise Quota                                                   | GÃ¼rÃ¼ltÃ¼ ekleme kotasÄ± terimi eklendi                                        |

---
*Bu belge proje ilerledikÃ§e gÃ¼ncellenecektir.* ğŸ”„
---