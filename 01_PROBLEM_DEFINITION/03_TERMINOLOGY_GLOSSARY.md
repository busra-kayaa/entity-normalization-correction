# ğŸ“š TERMINOLOGY GLOSSARY
## Entity Normalization & Correction

| **Son GÃ¼ncelleme** | 12.02.2026 |
| :--- |:-----------|
| **Toplam Terim** | 16         |
| **HazÄ±rlayan** | BÃ¼ÅŸra Kaya |

---

## ğŸ“‹ Ä°Ã‡Ä°NDEKÄ°LER

| # | BÃ¶lÃ¼m | Durum |
| :--- | :--- | :--- |
| 1 | [STEP 1 - PROBLEM TANIMI](#step-1---problem-tanimi) | âœ… |

---

## STEP 1 - PROBLEM TANIMI
*LiteratÃ¼r taramasÄ± ve problem tanÄ±mÄ± sÃ¼recinde Ã¶ÄŸrenilen terimler*

---

### ğŸ“Œ De-asciification

| | |
| :--- |:---|
| **ğŸ—“ï¸ Ne zaman?** | 11.02.2026 |
| **ğŸ“ Nerede?** | Problem tanÄ±mÄ±, YazÄ±m HatalarÄ± SÄ±nÄ±flandÄ±rmasÄ± |
| **â“ Ne iÅŸe yarar?** | ASCII karakterlere dÃ¶nÃ¼ÅŸmÃ¼ÅŸ TÃ¼rkÃ§e harfleri orijinal haline getirir |
| **ğŸ’¡ Basit Ã¶rnek** | `Turkiye` â†’ **`TÃ¼rkiye`**, `Istanbul` â†’ **`Ä°stanbul`**, `Erdogan` â†’ **`ErdoÄŸan`** |
| **ğŸ“š Benzer terimler** | Turkish character normalization, ASCII conversion, Unicode normalization |

---

### ğŸ“Œ OOV
*Out-of-Vocabulary*

| | |
| :--- |:---|
| **ğŸ—“ï¸ Ne zaman?** | 11.02.2026 |
| **ğŸ“ Nerede?** | Problem KÄ±sÄ±tlarÄ± (KÄ±sÄ±t 3) |
| **â“ Ne iÅŸe yarar?** | Modelin eÄŸitim verisinde **olmayan** kelimeleri ifade eder |
| **ğŸ’¡ Basit Ã¶rnek** | Yeni seÃ§ilen bir bakanÄ±n soyadÄ±, yeni kurulan bir ÅŸirket, yapay isimler |
| **ğŸ“š Benzer terimler** | Unknown words, Unseen tokens, Rare words |

---

### ğŸ“Œ Entity Normalization

| | |
| :--- | :--- |
| **ğŸ—“ï¸ Ne zaman?** | 11.02.2026 |
| **ğŸ“ Nerede?** | Problem tanÄ±mÄ±, YazÄ±m HatalarÄ± SÄ±nÄ±flandÄ±rmasÄ± |
| **â“ Ne iÅŸe yarar?** | AynÄ± varlÄ±ÄŸÄ±n farklÄ± yazÄ±mlarÄ±nÄ± standart forma getirir |
| **ğŸ’¡ Basit Ã¶rnek** | `Turkiye`, `Turkey`, `TURKIYE`, `TÃ¼rkiye Cumhuriyeti` â†’ **`TÃ¼rkiye`** |
| **ğŸ§  Neden gerekli?** | Bilgisayar `Apple`, `Apple Inc.`, `apple`'Ä± **4 farklÄ± ÅŸirket** sanar. Normalizasyon hepsini tek ID altÄ±nda birleÅŸtirir. |
| **ğŸ› ï¸ NasÄ±l Ã§alÄ±ÅŸÄ±r?** | **1. YazÄ±msal temizlik:** BÃ¼yÃ¼k/kÃ¼Ã§Ã¼k harf, noktalama, boÅŸluk <br> **2. SÃ¶zlÃ¼k eÅŸleÅŸtirme:** `ABD` = `USA` = `Amerika` <br> **3. Belirsizlik Ã§Ã¶zÃ¼mÃ¼:** "Amazon" â†’ orman mÄ±? ÅŸirket mi? |
| **ğŸ¯ Entity Linking farkÄ±?** | **Normalization:** Standart forma sokar (`RTE` â†’ `Recep Tayyip ErdoÄŸan`) <br> **Linking:** Bilgi bankasÄ±na baÄŸlar (`Recep Tayyip ErdoÄŸan` â†’ `wikidata.org/Q57418`) |
| **ğŸ“š Benzer terimler** | Entity resolution, Entity linking, Entity alignment, Name normalization, Record linkage |


---

### ğŸ“Œ NER
*Named Entity Recognition*

| |                                                                        |
| :--- |:-----------------------------------------------------------------------|
| **ğŸ—“ï¸ Ne zaman?** | 11.02.2026                                                             |
| **ğŸ“ Nerede?** | Problem tanÄ±mÄ±, Hata SÄ±nÄ±flandÄ±rmasÄ±                                   |
| **â“ Ne iÅŸe yarar?** | Metindeki Ã¶zel isimleri (kiÅŸi, yer, kurum) bulur ve kategorize eder    |
| **ğŸ’¡ Basit Ã¶rnek** | `"Joe Biden Washington'da"` â†’ `[Joe Biden: KÄ°ÅÄ°]`, `[Washington: YER]` |
| **ğŸ“š Benzer terimler** | NER tagging, IOB tagging, Span detection, Entity extraction            |

---

### ğŸ“Œ Context-Aware Spelling Correction

| | |
| :--- |:---|
| **ğŸ—“ï¸ Ne zaman?** | 11.02.2026 |
| **ğŸ“ Nerede?** | Problem tanÄ±mÄ±, Genel Ä°ngilizce YazÄ±m HatalarÄ± |
| **â“ Ne iÅŸe yarar?** | CÃ¼mlenin baÄŸlamÄ±na bakarak yazÄ±m hatalarÄ±nÄ± dÃ¼zeltir |
| **ğŸ’¡ Basit Ã¶rnek** | `"The goverment announced"` â†’ **`"The government announced"`** |
| | `"I have a brown cat"` â†’ âŒ **DÃ¼zeltmez** (Ã§Ã¼nkÃ¼ `"brown"` doÄŸru) |
| **ğŸ“š Benzer terimler** | Grammatical Error Correction (GEC), Spell checking, Proofreading |

### ğŸ“Œ Noisy Text Normalization

| | |
| :--- |:---|
| **ğŸ—“ï¸ Ne zaman?** | 11.02.2026 |
| **ğŸ“ Nerede?** | Problem tanÄ±mÄ±, De-asciification |
| **â“ Ne iÅŸe yarar?** | OCR, klavye, sosyal medya gibi kaynaklardan gelen **kirli metni** temizler |
| **ğŸ’¡ Basit Ã¶rnek** | `"Th3 c@p1t@l 0f Turkiye is Ankara."` â†’ **`"The capital of TÃ¼rkiye is Ankara."`** |
| **ğŸ“š Benzer terimler** | Text cleaning, Text preprocessing, De-asciification, Text denoising |

---

### ğŸ“Œ BERT
*Bidirectional Encoder Representations from Transformers*

| |                                                                                      |
| :--- |:-------------------------------------------------------------------------------------|
| **ğŸ—“ï¸ Ne zaman?** | 12.02.2026                                                                           |
| **ğŸ“ Nerede?** | LiteratÃ¼r taramasÄ±                                                                   |
| **â“ Ne iÅŸe yarar?** | Metindeki kelimeleri Ã§ift yÃ¶nlÃ¼ okuyarak baÄŸlamÄ± anlar                               |
| **ğŸ’¡ Basit Ã¶rnek** | `"ErdoÄŸan cumhurbaÅŸkanÄ±dÄ±r"` â†’ `"ErdoÄŸan"` kelimesinin kiÅŸi olduÄŸunu baÄŸlamdan anlar |
| **ğŸ“š Benzer terimler** | RoBERTa, ALBERT, DistilBERT, Transformer                                             |

---

### ğŸ“Œ GECToR
*Grammatical Error Correction: Tag, Not Rewrite*

| | |
| :--- | :--- |
| **ğŸ—“ï¸ Ne zaman?** | 12.02.2026 |
| **ğŸ“ Nerede?** | LiteratÃ¼r taramasÄ± |
| **â“ Ne iÅŸe yarar?** | Metni yeniden yazmak yerine **etiket atayarak** hata dÃ¼zeltir |
| **ğŸ’¡ Basit Ã¶rnek** | **Geleneksel yÃ¶ntem (Seq2Seq):** <br> `"goed"` â†’ `"went"` (tÃ¼m kelimeyi sil, yeni kelime yaz) <br><br> **GECToR yÃ¶ntemi:** <br> `"goed"` â†’ `"$APPEND_went"` (kelimeye etiket ekle, nasÄ±l dÃ¶nÃ¼ÅŸeceÄŸini sÃ¶yle) |
| **ğŸ§  NasÄ±l Ã§alÄ±ÅŸÄ±r?** | 1. BERT cÃ¼mleyi vektÃ¶rlere Ã§evirir <br> 2. Her kelime iÃ§in bir **etiket** tahmin eder <br> 3. Etiketler dÃ¶nÃ¼ÅŸÃ¼m kuralÄ±nÄ± belirtir: <br> &nbsp;&nbsp;&nbsp; â€¢ `$KEEP` â†’ olduÄŸu gibi bÄ±rak <br> &nbsp;&nbsp;&nbsp; â€¢ `$DELETE` â†’ sil <br> &nbsp;&nbsp;&nbsp; â€¢ `$APPEND_went` â†’ sonuna "went" ekle <br> &nbsp;&nbsp;&nbsp; â€¢ `$REPLACE_went` â†’ "went" ile deÄŸiÅŸtir |
| **âš¡ Seq2Seq'ten farkÄ±?** | **Seq2Seq:** TÃ¼m cÃ¼mleyi **sÄ±fÄ±rdan Ã¼retir** â†’ yavaÅŸ, pahalÄ±, Ã§ok parametre <br> **GECToR:** Sadece **hatalÄ± kÄ±sma etiket** atar â†’ hÄ±zlÄ±, hafif, az parametre |
| **ğŸ“Š Performans** | â€¢ Seq2Seq'ten **5-10 kat daha hÄ±zlÄ±** <br> â€¢ Daha az veriyle eÄŸitilir <br> â€¢ Benzer veya daha iyi doÄŸruluk |
| **ğŸ¯ Neden Ã¶nemli?** | GerÃ§ek zamanlÄ± dÃ¼zeltme yapabilir. Haber akÄ±ÅŸÄ±nda anÄ±nda Ã§alÄ±ÅŸÄ±r. |
| **ğŸ“š Benzer terimler** | Seq2seq, Transformer, Sequence tagging, BERT |

---

### ğŸ“Œ Transformer
*"Attention Is All You Need" - Vaswani et al. 2017*

| | |
| :--- | :--- |
| **ğŸ—“ï¸ Ne zaman?** | 12.02.2026 |
| **ğŸ“ Nerede?** | LiteratÃ¼r taramasÄ± |
| **â“ Ne iÅŸe yarar?** | **NLP'de devrim yaratan mimari.** <br> â€¢ 2017 Ã¶ncesi: LSTM/RNN (kelimeleri **tek tek** okur, yavaÅŸ) <br> â€¢ 2017 sonrasÄ±: Transformer (kelimeleri **aynÄ± anda** okur, hÄ±zlÄ±) |
| **ğŸ’¡ Basit Ã¶rnek** | **LSTM/RNN:** <br> `"Ben [bugÃ¼n] [okula] [gittim]"` <br> â†’ 1. "Ben" oku â†’ 2. "bugÃ¼n" oku â†’ 3. "okula" oku â†’ 4. "gittim" oku <br> âŒ Ä°lk kelimeyi okurken son kelimeyi bilmez <br> âŒ Uzun cÃ¼mlelerde baÅŸÄ± unutur <br><br> **Transformer:** <br> `"Ben bugÃ¼n okula gittim"` <br> â†’ **TÃ¼m kelimeleri AYNI ANDA gÃ¶rÃ¼r!** <br> âœ… "gittim" kelimesini okurken "Ben" kelimesini de gÃ¶rÃ¼r <br> âœ… BaÄŸlamÄ± tam anlar, hiÃ§bir ÅŸeyi unutmaz |
| **ğŸ§  Self-Attention nedir?** | Her kelimenin, cÃ¼mledeki **diÄŸer tÃ¼m kelimelerle** iliÅŸkisini hesaplama: <br><br> `"O bankaya para yatÄ±rdÄ±."` <br> â†’ "bankaya" kelimesi **"para"** ve **"yatÄ±rdÄ±"** ile gÃ¼Ã§lÃ¼ iliÅŸkili â†’ ğŸ¦ BANKA <br><br> `"Nehir kenarÄ±nda oturdu."` <br> â†’ "kenarÄ±nda" kelimesi **"nehir"** ile gÃ¼Ã§lÃ¼ iliÅŸkili â†’ ğŸŒŠ NEHÄ°R BANKASI <br><br> âœ… **AynÄ± kelime ("banka"), farklÄ± baÄŸlamlarda FARKLI vektÃ¶r temsili alÄ±r!** |
| **ğŸ—ï¸ Mimari yapÄ±sÄ±** | **Encoder-Decoder:** <br> â€¢ **Encoder:** Metni anlar, vektÃ¶re Ã§evirir (BERT) <br> â€¢ **Decoder:** VektÃ¶rden yeni metin Ã¼retir (GPT) <br><br> **Multi-head Attention (Ã‡ok baÅŸlÄ± dikkat):** <br> â€¢ 1. baÅŸ: Kelimelerin **gramer** iliÅŸkisini Ã¶ÄŸrenir <br> â€¢ 2. baÅŸ: Kelimelerin **anlam** iliÅŸkisini Ã¶ÄŸrenir <br> â€¢ 3. baÅŸ: Kelimelerin **konum** iliÅŸkisini Ã¶ÄŸrenir <br> â€¢ 4. baÅŸ: **Zamir** iliÅŸkisini Ã¶ÄŸrenir (o/onun) <br> â€¢ 5. baÅŸ: **ZÄ±tlÄ±k** iliÅŸkisini Ã¶ÄŸrenir <br> â€¢ ... 12+ baÅŸ farklÄ± Ã¶zellik Ã¶ÄŸrenir |
| **ğŸ“Š LSTM vs Transformer** | **LSTM/RNN:** <br> âŒ SÄ±ralÄ± iÅŸlem â†’ 100 kelime = 100 adÄ±m <br> âŒ Uzun cÃ¼mlelerde baÅŸÄ± unutur <br> âŒ Paralel iÅŸleme yok = GPU'yu tam kullanamaz <br> âŒ 500+ kelimede performans dÃ¼ÅŸer <br><br> **Transformer:** <br> âœ… Paralel iÅŸlem â†’ 100 kelime = 1 adÄ±m <br> âœ… 1000+ kelimeyi de hatÄ±rlar <br> âœ… GPU'yu %100 kullanÄ±r, Ã§ok hÄ±zlÄ± <br> âœ… 10.000 kelimeye kadar Ã§Ä±kabilir |
| **ğŸ”§ Kullanan modeller** | â€¢ **BERT** - Sadece Encoder (metin anlama) <br> â€¢ **GPT** - Sadece Decoder (metin Ã¼retme) <br> â€¢ **T5** - Encoder + Decoder (Ã§eviri, Ã¶zet) <br> â€¢ **BART** - Encoder + Decoder (metin onarma) <br> â€¢ **RoBERTa** - BERT'in geliÅŸmiÅŸi <br> â€¢ **GECToR** - BERT + etiketleme |
| **ğŸ“š Benzer terimler** | Self-attention, Multi-head attention, Encoder-Decoder, Positional encoding, BERT, GPT |


---


### ğŸ“Œ Fine-tuning

| | |
| :--- | :--- |
| **ğŸ—“ï¸ Ne zaman?** | 12.02.2026 |
| **ğŸ“ Nerede?** | LiteratÃ¼r taramasÄ± |
| **â“ Ne iÅŸe yarar?** | Ã–nceden eÄŸitilmiÅŸ modeli yeni bir veri setiyle Ã¶zelleÅŸtirme |
| **ğŸ’¡ Basit Ã¶rnek** | Genel BERT â†’ Haber metinleriyle eÄŸitilmiÅŸ BERT |
| **ğŸ“š Benzer terimler** | Transfer learning, Pre-training, Domain adaptation |

---

### ğŸ“Œ OCR
*Optical Character Recognition*

| | |
| :--- | :--- |
| **ğŸ—“ï¸ Ne zaman?** | 12.02.2026 |
| **ğŸ“ Nerede?** | LiteratÃ¼r taramasÄ±, Noisy Text Normalization |
| **â“ Ne iÅŸe yarar?** | TaranmÄ±ÅŸ belgeler, PDF'ler veya fotoÄŸraflardaki yazÄ±larÄ± dijital metne Ã§evirir |
| **ğŸ’¡ Basit Ã¶rnek** | TaranmÄ±ÅŸ bir gazete kupÃ¼rÃ¼ â†’ `"TÃ¼rkiye"` yazÄ±sÄ± bilgisayarda **dÃ¼zenlenebilir metin** olur |
| **âš ï¸ OCR hatalarÄ±** | **Harf karÄ±ÅŸÄ±klÄ±klarÄ±:** <br> â€¢ `"TÃ¼rkiye"` â†’ `"Turkiye"` (Ã¼â†’u) <br> â€¢ `"Ä°stanbul"` â†’ `"Istanbul"` (Ä°â†’I) <br> â€¢ `"ErdoÄŸan"` â†’ `"Erdogan"` (ÄŸâ†’g) <br><br> **Benzer harfler:** <br> â€¢ `"O"` ve `"0"` karÄ±ÅŸmasÄ± <br> â€¢ `"l"` (kÃ¼Ã§Ã¼k L) ve `"1"` (bir) karÄ±ÅŸmasÄ± <br> â€¢ `"rn"` â†’ `"m"` olarak okunmasÄ± |
| **ğŸ§  Projeyle iliÅŸkisi** | OCR hatalarÄ± = **Noisy Text**'in ana kaynaklarÄ±ndan biri <br> â€¢ Gazete arÅŸivleri (taranmÄ±ÅŸ) <br> â€¢ Eski belgeler, PDF'ler <br> â€¢ DijitalleÅŸtirilmiÅŸ haber metinleri |
| **ğŸ”§ Ã‡Ã¶zÃ¼m** | **De-asciification** + **Context-Aware Spelling Correction** ile OCR hatalarÄ± dÃ¼zeltilir |
| **ğŸ“š Benzer terimler** | Document scanning, Text digitization, Document analysis, OCR post-correction |

---

### ğŸ“Œ Multi-Head Attention
*Ã‡ok BaÅŸlÄ± Dikkat MekanizmasÄ±*

| | |
| :--- | :--- |
| **ğŸ—“ï¸ Ne zaman?** | 17.02.2026 |
| **ğŸ“ Nerede?** | LiteratÃ¼r taramasÄ±, Transformer mimarisi |
| **â“ Ne iÅŸe yarar?** | CÃ¼mledeki kelimeler arasÄ±ndaki **farklÄ± iliÅŸki tÃ¼rlerini** aynÄ± anda Ã¶ÄŸrenir |
| **ğŸ’¡ Basit Ã¶rnek** | Bir cÃ¼mleyi **farklÄ± uzmanlara** aynÄ± anda inceletmek gibi: <br> â€¢ 1. uzman: Kim kimi iÅŸaret ediyor? (zamirler) <br> â€¢ 2. uzman: Hangi kelimeler anlamca yakÄ±n? <br> â€¢ 3. uzman: CÃ¼mledeki zÄ±tlÄ±klar neler? <br> â€¢ 4. uzman: Dil bilgisi yapÄ±sÄ± nasÄ±l? |
| **ğŸ§  Neden gerekli?** | Tek bir dikkat mekanizmasÄ± **her ÅŸeyi aynÄ± anda** Ã¶ÄŸrenmek zorunda kalÄ±r. <br> Ã‡ok baÅŸlÄ± dikkat ile **her baÅŸ farklÄ± bir Ã¶zelliÄŸe odaklanÄ±r**, sonra hepsi birleÅŸtirilir. |
| **ğŸ› ï¸ NasÄ±l Ã§alÄ±ÅŸÄ±r?** | **AdÄ±m 1:** Girdi cÃ¼mlesi 8 farklÄ± kopyaya Ã§oÄŸaltÄ±lÄ±r <br> **AdÄ±m 2:** Her kopya farklÄ± bir "dikkat baÅŸlÄ±ÄŸÄ±" tarafÄ±ndan iÅŸlenir <br> **AdÄ±m 3:** Her baÅŸ farklÄ± iliÅŸkiler Ã¶ÄŸrenir <br> **AdÄ±m 4:** TÃ¼m baÅŸlarÄ±n Ã§Ä±ktÄ±larÄ± birleÅŸtirilir |
| **ğŸ”§ Transformer'daki yeri** | â€¢ **BERT:** 12 baÅŸlÄ± dikkat (base model) <br> â€¢ **BERT-large:** 16 baÅŸlÄ± dikkat <br> â€¢ **GPT-3:** 96 baÅŸlÄ± dikkat |
| **ğŸ“š Benzer terimler** | Self-attention, Transformer, Attention mechanism, Encoder-Decoder attention |

---
### ğŸ“Œ Positional Encoding
*Konumsal Kodlama / Pozisyonel Kodlama*

| | |
| :--- | :--- |
| **ğŸ—“ï¸ Ne zaman?** | 17.02.2026 |
| **ğŸ“ Nerede?** | LiteratÃ¼r taramasÄ±, Transformer mimarisi |
| **â“ Ne iÅŸe yarar?** | Transformer modeline **kelimelerin cÃ¼mle iÃ§indeki sÄ±rasÄ±nÄ±** Ã¶ÄŸretir |
| **ğŸ’¡ Basit Ã¶rnek** | **Transformer olmadan Ã¶nce (LSTM/RNN):** <br> Kelimeler sÄ±rayla okunur â†’ sÄ±ra bilgisi **otomatik** Ã¶ÄŸrenilir <br><br> **Transformer'da:** <br> TÃ¼m kelimeler **aynÄ± anda** okunur â†’ "Hangi kelime Ã¶nce geldi?" bilgisini kaybeder <br> â†’ **Positional Encoding** kelimelere "Ben 1. kelimeyim", "Ben 2. kelimeyim" etiketi yapÄ±ÅŸtÄ±rÄ±r |
| **ğŸ§  Neden gerekli?** | `"KÃ¶pek adamÄ± Ä±sÄ±rdÄ±"` ile `"Adam kÃ¶peÄŸi Ä±sÄ±rdÄ±"` aynÄ± kelimeler ama **tam tersi anlam**! <br> SÄ±ra bilgisi olmadan Transformer bunlarÄ± **ayÄ±rt edemez**. |
| **ğŸ› ï¸ NasÄ±l Ã§alÄ±ÅŸÄ±r?** | **AdÄ±m 1:** Her kelimeye bir vektÃ¶r atanÄ±r (anlamÄ±) <br> **AdÄ±m 2:** Kelimenin cÃ¼mledeki sÄ±rasÄ±na gÃ¶re bir **pozisyon vektÃ¶rÃ¼** hesaplanÄ±r <br> **AdÄ±m 3:** Anlam vektÃ¶rÃ¼ + Pozisyon vektÃ¶rÃ¼ = Transformer'a giren **son vektÃ¶r** <br><br> `["Ben", "okula", "gidiyorum"]` <br> â†’ "Ben" = anlam vektÃ¶rÃ¼ + (1. pozisyon vektÃ¶rÃ¼) <br> â†’ "okula" = anlam vektÃ¶rÃ¼ + (2. pozisyon vektÃ¶rÃ¼) <br> â†’ "gidiyorum" = anlam vektÃ¶rÃ¼ + (3. pozisyon vektÃ¶rÃ¼) |
| **ğŸ“ FormÃ¼l (Teknik)** | **SinÃ¼zoidal fonksiyonlar** kullanÄ±lÄ±r: <br> â€¢ Ã‡ift indeksler iÃ§in: `sin(pos/10000^(2i/d))` <br> â€¢ Tek indeksler iÃ§in: `cos(pos/10000^(2i/d))` <br><br> *Sebep:* SinÃ¼s ve kosinÃ¼s sayesinde model **gÃ¶receli pozisyonlarÄ±** (2. kelime ile 5. kelime arasÄ±) Ã¶ÄŸrenebilir. |
| **ğŸ”§ Alternatifler** | **Ã–ÄŸrenilebilir Positional Encoding:** <br> â€¢ SinÃ¼zoidal formÃ¼l yerine modelin **kendi Ã¶ÄŸrenmesi** saÄŸlanÄ±r <br> â€¢ GPT gibi modeller bunu kullanÄ±r <br><br> **Relative Positional Encoding:** <br> â€¢ Mutlak sÄ±ra yerine kelimelerin **birbirine gÃ¶re mesafesi** Ã¶ÄŸrenilir |
| **ğŸ“š Benzer terimler** | Self-attention, Transformer, Position embedding, Sinusoidal encoding, Relative position bias |

---
### ğŸ“Œ Masked Language Model (MLM)
*Maskeli Dil Modeli*

| | |
| :--- | :--- |
| **ğŸ—“ï¸ Ne zaman?** | 17.02.2026 |
| **ğŸ“ Nerede?** | LiteratÃ¼r taramasÄ±, BERT mimarisi |
| **â“ Ne iÅŸe yarar?** | BERT'in eÄŸitim yÃ¶ntemi. CÃ¼mledeki bazÄ± kelimeleri **maskeler** ve modelden **tahmin etmesini** ister. |
| **ğŸ’¡ Basit Ã¶rnek** | **Orijinal cÃ¼mle:** `"Ä°stanbul TÃ¼rkiye'nin baÅŸkenti deÄŸildir"` <br> **MaskelenmiÅŸ:** `"Ä°stanbul TÃ¼rkiye'nin [MASK] deÄŸildir"` <br> **BERT'in tahmini:** `"baÅŸkenti"` âœ… |
| **ğŸ§  Neden gerekli?** | **Geleneksel modeller (GPT):** Soldan saÄŸa okur, bir sonraki kelimeyi tahmin eder <br> â†’ `"Ben okula ___"` â†’ `"gidiyorum"` <br><br> **BERT'in yaptÄ±ÄŸÄ±:** Ã‡ift yÃ¶nlÃ¼ okur, **ortadaki** kelimeyi tahmin eder <br> â†’ `"Ben ___ gidiyorum"` â†’ `"okula"` |
| **ğŸ› ï¸ NasÄ±l Ã§alÄ±ÅŸÄ±r?** | **AdÄ±m 1:** CÃ¼mledeki kelimelerin **%15'i** rastgele seÃ§ilir <br> **AdÄ±m 2:** SeÃ§ilen kelimelerin: <br> &nbsp;&nbsp;&nbsp; â€¢ %80'i `[MASK]` ile deÄŸiÅŸtirilir <br> &nbsp;&nbsp;&nbsp; â€¢ %10'u rastgele baÅŸka bir kelimeyle deÄŸiÅŸtirilir <br> &nbsp;&nbsp;&nbsp; â€¢ %10'u **aynen korunur** <br> **AdÄ±m 3:** Model maskelenen yerde **hangi kelime olmasÄ± gerektiÄŸini** tahmin eder |
| **ğŸ§ª Neden %100 MASK kullanÄ±lmÄ±yor?** | EÄŸer hep `[MASK]` gÃ¶rÃ¼rse, BERT **gerÃ§ek metinlerde** mask gÃ¶rmeyeceÄŸi iÃ§in ÅŸaÅŸÄ±rÄ±r. <br> â€¢ %80 MASK â†’ Maskeyi tahmin etmeyi Ã¶ÄŸrenir <br> â€¢ %10 rastgele kelime â†’ HatalarÄ± dÃ¼zeltmeyi Ã¶ÄŸrenir <br> â€¢ %10 aynÄ± kelime â†’ Her zaman deÄŸiÅŸmeyeceÄŸini de Ã¶ÄŸrenir |
| **ğŸ”§ BERT'ten sonra gelen modeller** | **RoBERTa:** %15 MASK, ara **hiÃ§ rastgele veya aynÄ± kelime yok**, sadece MASK! <br> **SpanBERT:** Tek kelime yerine **kelime gruplarÄ±nÄ±** maskeler <br> **ELECTRA:** Maskeli kelimeyi tahmin etmek yerine, **hangi kelimenin deÄŸiÅŸtirildiÄŸini** bulur |
| **ğŸ“š Benzer terimler** | Next Sentence Prediction (NSP), Autoregressive model, Denoising autoencoder, SpanBERT, RoBERTa |

---

### ğŸ“Œ Next Sentence Prediction (NSP)
*Sonraki CÃ¼mle Tahmini*

| | |
| :--- | :--- |
| **ğŸ—“ï¸ Ne zaman?** | 17.02.2026 |
| **ğŸ“ Nerede?** | LiteratÃ¼r taramasÄ±, BERT mimarisi |
| **â“ Ne iÅŸe yarar?** | Ä°ki cÃ¼mle arasÄ±ndaki iliÅŸkiyi Ã¶ÄŸrenir. Ä°kinci cÃ¼mlenin birinciyi takip edip etmediÄŸini tahmin eder. |
| **ğŸ’¡ Basit Ã¶rnek** | **CÃ¼mle A:** `"Ä°stanbul Ã§ok kalabalÄ±k bir ÅŸehir."` <br> **CÃ¼mle B:** `"Her gÃ¼n milyonlarca insan trafikte zaman kaybediyor."` <br> **Tahmin:** `IsNext` âœ… (Birbiriyle ilgili) <br><br> **CÃ¼mle A:** `"Ä°stanbul Ã§ok kalabalÄ±k bir ÅŸehir."` <br> **CÃ¼mle B:** `"Kediler Ã§ok sevimli hayvanlardÄ±r."` <br> **Tahmin:** `NotNext` âŒ (AlakasÄ±z) |
| **ğŸ“š Benzer terimler** | Sentence order prediction, Document-level understanding |

---

### ğŸ“Œ Embeddings from Language Models (ELMo)
*Dil Modellerinden Elde Edilen GÃ¶mmeler*

| | |
| :--- | :--- |
| **ğŸ—“ï¸ Ne zaman?** | 17.02.2026 |
| **ğŸ“ Nerede?** | LiteratÃ¼r taramasÄ± |
| **â“ Ne iÅŸe yarar?** | Kelimelere **cÃ¼mle iÃ§indeki baÄŸlama gÃ¶re** vektÃ¶r atar. AynÄ± kelime farklÄ± cÃ¼mlelerde farklÄ± vektÃ¶r temsili alÄ±r. |
| **ğŸ’¡ Basit Ã¶rnek** | **CÃ¼mle 1:** `"I read the book yesterday."` (read = geÃ§miÅŸ zaman) <br> **CÃ¼mle 2:** `"Can you read the letter now?"` (read = ÅŸimdiki zaman) <br><br> **Word2vec/GloVe:** Ä°ki cÃ¼mledeki "read" kelimesine **aynÄ±** vektÃ¶rÃ¼ verir âŒ <br> **ELMo:** Ä°ki cÃ¼mledeki "read" kelimesine **farklÄ±** vektÃ¶rler verir âœ… |
| **ğŸ§  Neden gerekli?** | Word2vec gibi geleneksel yÃ¶ntemler bir kelimenin **tÃ¼m anlamlarÄ±nÄ±** tek bir vektÃ¶re sÄ±kÄ±ÅŸtÄ±rÄ±r. <br> `"banka"` kelimesi: <br> â€¢ "Para Ã§ekmek iÃ§in bankaya gittim" â†’ ğŸ¦ <br> â€¢ "Nehir bankasÄ±nda oturduk" â†’ ğŸŒŠ <br><br> ELMo, cÃ¼mlenin tamamÄ±nÄ± okuyarak kelimenin **o cÃ¼mledeki anlamÄ±nÄ±** yakalar. |
| **ğŸ› ï¸ NasÄ±l Ã§alÄ±ÅŸÄ±r?** | **AdÄ±m 1:** Ã‡ift yÃ¶nlÃ¼ LSTM (biLM) eÄŸitilir: <br> &nbsp;&nbsp;&nbsp; â€¢ Ä°leri LSTM: soldan saÄŸa okuyarak sonraki kelimeyi tahmin eder <br> &nbsp;&nbsp;&nbsp; â€¢ Geri LSTM: saÄŸdan sola okuyarak Ã¶nceki kelimeyi tahmin eder <br> **AdÄ±m 2:** Her kelime iÃ§in 3 katman vektÃ¶r Ã¼retilir: <br> &nbsp;&nbsp;&nbsp; â€¢ Katman 0: Kelimenin kendi embedding'i <br> &nbsp;&nbsp;&nbsp; â€¢ Katman 1: Ä°lk LSTM katmanÄ± Ã§Ä±ktÄ±sÄ± (dil bilgisi, syntax) <br> &nbsp;&nbsp;&nbsp; â€¢ Katman 2: Ä°kinci LSTM katmanÄ± Ã§Ä±ktÄ±sÄ± (anlam, semantics) <br> **AdÄ±m 3:** Bu 3 katmanÄ±n **aÄŸÄ±rlÄ±klÄ± ortalamasÄ±** alÄ±nÄ±r (aÄŸÄ±rlÄ±klar gÃ¶reve gÃ¶re Ã¶ÄŸrenilir) |
| **ğŸ¯ KatmanlarÄ±n gÃ¶revi** | â€¢ **Alt katman (LSTM1):** Dil bilgisi, sÃ¶zdizimi (POS tagging, dependency) <br> â€¢ **Ãœst katman (LSTM2):** Anlam, baÄŸlam (sentiment, QA, NER) <br><br> *Not:* GÃ¶rev neyse, o gÃ¶rev iÃ§in hangi katman daha Ã¶nemliyse aÄŸÄ±rlÄ±ÄŸÄ± artar. |
| **âš™ï¸ Teknik detay** | â€¢ Karakter seviyesinde CNN ile kelime temsili (OOV sorununa Ã§Ã¶zÃ¼m) <br> â€¢ 2 katman biLSTM <br> â€¢ 4096 hidden unit, 512 projection <br> â€¢ 1 milyar kelime benchmark verisiyle eÄŸitilmiÅŸ |
| **ğŸ“š Benzer terimler** | BiLM (Bidirectional Language Model), Contextual embeddings, CoVe, LSTM, Character CNN |

---

## ğŸ“Š Ã–ZET TABLOSU

| Terim | KÄ±saltma | Ã–ÄŸrenme Tarihi |
| :--- |:---------| :--- |
| De-asciification | -        | 11.02.2026 |
| Out-of-Vocabulary | **OOV**  | 11.02.2026 |
| Entity Normalization | -        | 11.02.2026 |
| Named Entity Recognition | **NER**  | 11.02.2026 |
| Context-Aware Spelling Correction | -        | 11.02.2026 |
| Noisy Text Normalization | -        | 11.02.2026 |
| BERT | -        | 12.02.2026 |
| GECToR | -        | 12.02.2026 |
| Transformer | -        | 12.02.2026 |
| Fine-tuning | -        | 12.02.2026 |
| OCR | -        | 12.02.2026 |
| Multi-Head Attention | -        | 17.02.2026 |
| Positional Encoding | -        | 17.02.2026 |
| Masked Language Model (MLM) | **MLM**  | 17.02.2026 |
| Next Sentence Prediction (NSP) | **NSP**  | 17.02.2026 |
| Embeddings from Language Models | **ELMo**     | 17.02.2026 |

---

## ğŸ“Œ DEÄÄ°ÅÄ°KLÄ°K KAYITLARI

| Tarih | Versiyon | Eklenen Terimler                                              | AÃ§Ä±klama |
| :--- |:---------|:--------------------------------------------------------------| :--- |
| 11.02.2026 | v1.0     | OOV, NER, Entity Norm, Spelling, Noisy Text, De-asciification | Ä°lk oluÅŸturma |
| 12.02.2026 | v1.1     | BERT, GECToR, Transformer, Fine-tuning, OCR                   | LiteratÃ¼r taramasÄ± eklendi |
| 17.02.2026 | v1.2     | Multi-Head Attention, Positional Encoding                     | Transformer detaylandÄ±rÄ±ldÄ± |
| 17.02.2026 | v1.3     | MLM, NSP                                                      | BERT eÄŸitim yÃ¶ntemi eklendi |
| 17.02.2026 | v1.4     | ELMo                                                          | BaÄŸlamsal embedding modeli eklendi |
---

*Bu belge proje ilerledikÃ§e gÃ¼ncellenecektir.* ğŸ”„

---